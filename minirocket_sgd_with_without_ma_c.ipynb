{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvn0EvMb2bsE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUS_nu452C5x",
        "outputId": "d2d24d6a-99fc-4953-d593-e4bd2b38fe22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEDqV7KI2F0y",
        "outputId": "b70feb5a-c08c-49ed-fe5f-77db1fd44aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns: ['time', 'ws', 'tp', 'hs', 'pwr', 'mslp', 'temp', 'surge', 'twl', 'steep', 'dwd_sin', 'dwd_cos', 'hs_ma3', 'hs_ma6', 'hs_ma12', 'ws_ma3', 'ws_ma6', 'ws_ma12', 'pwr_ma3', 'pwr_ma6', 'pwr_ma12', 'mslp_ma3', 'mslp_ma6', 'mslp_ma12', 'temp_ma3', 'temp_ma6', 'temp_ma12', 'surge_ma3', 'surge_ma6', 'surge_ma12', 'twl_ma3', 'twl_ma6', 'twl_ma12', 'steep_ma3', 'steep_ma6', 'steep_ma12', 'tp_ma3', 'tp_ma6', 'tp_ma12']\n",
            "Class dist:\n",
            "train {0: 0.725734690152414, 1: 0.1796986705606766, 2: 0.07084131909585957, 3: 0.017933740987496578, 4: 0.005791579203553284}\n",
            "val {0: 0.7295774005111354, 1: 0.17819003285870755, 2: 0.06863818912011684, 3: 0.015950164293537787, 4: 0.007644213216502373}\n",
            "test {0: 0.735375345217173, 1: 0.18250291009517722, 2: 0.060712573893593226, 3: 0.016889964165886836, 4: 0.0045192066281697215}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.lib.stride_tricks as st\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Paths and basic configuration\n",
        "# --------------------------------------------------\n",
        "PATH = \"/content/drive/MyDrive/Datamining-TSC-Project/new_processed_data.parquet\"\n",
        "CFG = {\n",
        "    \"time_col\": \"time\",\n",
        "    \"window\": 36,        # sliding window length (hours)\n",
        "    \"trend_h\": 12,       # recent hours for trend checks\n",
        "    \"ma_hours\": [3, 6, 12],  # moving average windows\n",
        "}\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Angle utility functions\n",
        "# --------------------------------------------------\n",
        "def wrap360(x):\n",
        "    # Wrap angles into [0, 360)\n",
        "    return (x % 360.0 + 360.0) % 360.0\n",
        "\n",
        "def angle_diff_deg(a, b):\n",
        "    # Smallest signed angle difference a - b in degrees\n",
        "    return (a - b + 180.0) % 360.0 - 180.0\n",
        "\n",
        "def wave_dir_convert(old_wave_dir):\n",
        "    # Convert wave direction to wind-direction convention\n",
        "    return wrap360(270.0 - old_wave_dir)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Moving average feature generator\n",
        "# --------------------------------------------------\n",
        "def add_moving_averages(df, cols, ma_hours):\n",
        "    # Add rolling mean features for selected columns\n",
        "    for col in cols:\n",
        "        for h in ma_hours:\n",
        "            df[f\"{col}_ma{h}\"] = df[col].rolling(window=h, min_periods=h).mean()\n",
        "    return df\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Load and clean data\n",
        "# --------------------------------------------------\n",
        "df = pd.read_parquet(PATH)\n",
        "\n",
        "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "df = df.sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "# Keep only relevant columns\n",
        "df = df[\n",
        "    [\n",
        "        \"time\",\n",
        "        \"Wind speed\",\n",
        "        \"Wind Direction\",\n",
        "        \"Wave Period\",\n",
        "        \"Wave Direction\",\n",
        "        \"Wave Height\",\n",
        "        \"Wave Power\",\n",
        "        \"Pressure\",\n",
        "        \"temperature\",\n",
        "        \"Surge Height\",\n",
        "        \"Total Water Level\",\n",
        "        \"Wave Steepness\",\n",
        "    ]\n",
        "].copy()\n",
        "\n",
        "# Rename columns to short, consistent names\n",
        "df.rename(\n",
        "    columns={\n",
        "        \"Wind speed\": \"ws\",\n",
        "        \"Wind Direction\": \"wd\",\n",
        "        \"Wave Period\": \"tp\",\n",
        "        \"Wave Direction\": \"wdir\",\n",
        "        \"Wave Height\": \"hs\",\n",
        "        \"Wave Power\": \"pwr\",\n",
        "        \"Pressure\": \"mslp\",\n",
        "        \"temperature\": \"temp\",\n",
        "        \"Surge Height\": \"surge\",\n",
        "        \"Total Water Level\": \"twl\",\n",
        "        \"Wave Steepness\": \"steep\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Wind–wave direction alignment features\n",
        "# --------------------------------------------------\n",
        "df[\"wdir\"] = wave_dir_convert(df[\"wdir\"].to_numpy(np.float32))\n",
        "\n",
        "wd = df[\"wd\"].to_numpy(np.float32)\n",
        "wdir = df[\"wdir\"].to_numpy(np.float32)\n",
        "\n",
        "# Angle difference between wind and wave directions\n",
        "dwd_deg = angle_diff_deg(wd, wdir).astype(np.float32)\n",
        "dwd_rad = np.deg2rad(dwd_deg).astype(np.float32)\n",
        "\n",
        "# Encode direction difference with sin/cos\n",
        "df[\"dwd_sin\"] = np.sin(dwd_rad).astype(np.float32)\n",
        "df[\"dwd_cos\"] = np.cos(dwd_rad).astype(np.float32)\n",
        "\n",
        "# Drop raw direction columns\n",
        "df.drop(columns=[\"wd\", \"wdir\"], inplace=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Add moving average features\n",
        "# --------------------------------------------------\n",
        "ma_cols = [\n",
        "    \"hs\", \"ws\", \"pwr\", \"mslp\",\n",
        "    \"temp\", \"surge\", \"twl\", \"steep\", \"tp\"\n",
        "]\n",
        "df = add_moving_averages(df, ma_cols, CFG[\"ma_hours\"])\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Sliding window statistics\n",
        "# --------------------------------------------------\n",
        "W = CFG[\"window\"]\n",
        "H = CFG[\"trend_h\"]\n",
        "\n",
        "hs   = df[\"hs\"].to_numpy(np.float32)\n",
        "pwr  = df[\"pwr\"].to_numpy(np.float32)\n",
        "mslp = df[\"mslp\"].to_numpy(np.float32)\n",
        "ws   = df[\"ws\"].to_numpy(np.float32)\n",
        "dwd_cos = df[\"dwd_cos\"].to_numpy(np.float32)\n",
        "\n",
        "# Create rolling windows\n",
        "hs_w   = st.sliding_window_view(hs,   W)\n",
        "pwr_w  = st.sliding_window_view(pwr,  W)\n",
        "mslp_w = st.sliding_window_view(mslp, W)\n",
        "ws_w   = st.sliding_window_view(ws,   W)\n",
        "dwd_cos_w = st.sliding_window_view(dwd_cos, W)\n",
        "\n",
        "# Window-based severity metrics (mean + 2*std)\n",
        "hs_metric36  = hs_w.mean(axis=1)  + 2.0 * hs_w.std(axis=1)\n",
        "pwr_metric36 = pwr_w.mean(axis=1) + 2.0 * pwr_w.std(axis=1)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Train / validation / test split by time\n",
        "# --------------------------------------------------\n",
        "start_times = df[\"time\"].iloc[:len(hs_metric36)].to_numpy()\n",
        "\n",
        "train_mask = start_times < np.datetime64(\"2015-01-01\")\n",
        "val_mask   = (start_times >= np.datetime64(\"2015-01-01\")) & (start_times < np.datetime64(\"2020-01-01\"))\n",
        "test_mask  = start_times >= np.datetime64(\"2020-01-01\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Percentile-based severity thresholds (train only)\n",
        "# --------------------------------------------------\n",
        "hs_p75, hs_p92, hs_p98, hs_p995 = np.percentile(\n",
        "    hs_metric36[train_mask], [75, 92, 98, 99.5]\n",
        ")\n",
        "pwr_p75, pwr_p92, pwr_p98, pwr_p995 = np.percentile(\n",
        "    pwr_metric36[train_mask], [75, 92, 98, 99.5]\n",
        ")\n",
        "\n",
        "# Map continuous values to severity classes\n",
        "def severity(x, p75, p92, p98, p995):\n",
        "    y = np.zeros_like(x, dtype=np.int8)\n",
        "    y[(x >= p75) & (x < p92)]  = 1\n",
        "    y[(x >= p92) & (x < p98)]  = 2\n",
        "    y[(x >= p98) & (x < p995)] = 3\n",
        "    y[(x >= p995)]             = 4\n",
        "    return y\n",
        "\n",
        "sev_hs  = severity(hs_metric36,  hs_p75,  hs_p92,  hs_p98,  hs_p995)\n",
        "sev_pwr = severity(pwr_metric36, pwr_p75, pwr_p92, pwr_p98, pwr_p995)\n",
        "\n",
        "# Base severity: worst of wave height or power\n",
        "base = np.maximum(sev_hs, sev_pwr)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Trend-based reinforcement rules (train only)\n",
        "# --------------------------------------------------\n",
        "train_hours_mask = df[\"time\"] < \"2015-01-01\"\n",
        "\n",
        "hs_th   = np.percentile(hs[train_hours_mask],  92)\n",
        "pwr_th  = np.percentile(pwr[train_hours_mask], 92)\n",
        "ws_th   = np.percentile(ws[train_hours_mask],  92)\n",
        "mslp_th = np.percentile(mslp[train_hours_mask], 20)\n",
        "align_th = np.percentile(dwd_cos[train_hours_mask], 75)\n",
        "\n",
        "# Count how many storm-like conditions persist in last H hours\n",
        "cnt = (\n",
        "    (hs_w[:, -H:]  >= hs_th).sum(axis=1)  >= 6\n",
        ").astype(int) + (\n",
        "    (pwr_w[:, -H:] >= pwr_th).sum(axis=1) >= 6\n",
        ").astype(int) + (\n",
        "    (ws_w[:, -H:]  >= ws_th).sum(axis=1)  >= 6\n",
        ").astype(int) + (\n",
        "    (mslp_w[:, -H:] <= mslp_th).sum(axis=1) >= 6\n",
        ").astype(int) + (\n",
        "    (dwd_cos_w[:, -H:] >= align_th).sum(axis=1) >= 4\n",
        ").astype(int)\n",
        "\n",
        "# Final labels (trend count can be used later if needed)\n",
        "y = base.copy()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Sanity checks\n",
        "# --------------------------------------------------\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"Class dist:\")\n",
        "for name, m in [(\"train\", train_mask), (\"val\", val_mask), (\"test\", test_mask)]:\n",
        "    print(name, pd.Series(y[m]).value_counts(normalize=True).sort_index().to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWKVWInN2HSZ",
        "outputId": "ea2f7f79-240b-46dd-8434-49a1f1746b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sktime in /usr/local/lib/python3.12/dist-packages (0.40.1)\n",
            "Requirement already satisfied: joblib<1.6,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from sktime) (1.5.3)\n",
            "Requirement already satisfied: numpy<2.4,>=1.21 in /usr/local/lib/python3.12/dist-packages (from sktime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from sktime) (25.0)\n",
            "Requirement already satisfied: pandas<2.4.0,>=1.1 in /usr/local/lib/python3.12/dist-packages (from sktime) (2.2.2)\n",
            "Requirement already satisfied: scikit-base<0.14.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from sktime) (0.13.0)\n",
            "Requirement already satisfied: scikit-learn<1.8.0,>=0.24 in /usr/local/lib/python3.12/dist-packages (from sktime) (1.6.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.12/dist-packages (from sktime) (1.16.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=1.1->sktime) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=1.1->sktime) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=1.1->sktime) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8.0,>=0.24->sktime) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.4.0,>=1.1->sktime) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sktime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI_f86ZVB7xP"
      },
      "source": [
        "MA YOK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SAq1-TZ2aIU",
        "outputId": "72fd1da0-4ddf-4522-d980-9df49d1e82f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using RAW features (no MAs): ['ws', 'tp', 'mslp', 'surge', 'twl', 'steep', 'temp', 'dwd_sin', 'dwd_cos']\n",
            "Shapes: \n",
            "  X_train: (262968, 36, 9) \n",
            "  X_val:   (43824, 36, 9) \n",
            "  X_test:  (43813, 36, 9) \n",
            "  #classes: 5\n",
            "Standardization: ON (train-only).\n",
            "Fitting MiniROCKET on TRAIN ...\n",
            "Transforming ...\n",
            "Transformed shapes: (262968, 9996) (43824, 9996) (43813, 9996)\n",
            "Selecting alpha on VAL (macro-F1) ...\n",
            "  alpha=1.00e-06 | val_f1_macro=0.5723\n",
            "  alpha=3.16e-06 | val_f1_macro=0.5439\n",
            "  alpha=1.00e-05 | val_f1_macro=0.4986\n",
            "  alpha=3.16e-05 | val_f1_macro=0.4799\n",
            "  alpha=1.00e-04 | val_f1_macro=0.5306\n",
            "  alpha=3.16e-04 | val_f1_macro=0.5381\n",
            "  alpha=1.00e-03 | val_f1_macro=0.5444\n",
            "  alpha=3.16e-03 | val_f1_macro=0.5303\n",
            "  alpha=1.00e-02 | val_f1_macro=0.5567\n",
            "Best alpha: 1.00e-06 | best val_f1_macro=0.5723\n",
            "\n",
            "TRAIN\n",
            "F1-macro: 0.5653308437628668\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.99      0.93    190845\n",
            "           1       0.70      0.43      0.53     47255\n",
            "           2       0.66      0.51      0.57     18629\n",
            "           3       0.60      0.15      0.24      4716\n",
            "           4       0.50      0.61      0.55      1523\n",
            "\n",
            "    accuracy                           0.84    262968\n",
            "   macro avg       0.67      0.54      0.57    262968\n",
            "weighted avg       0.82      0.84      0.82    262968\n",
            "\n",
            "Confusion matrix:\n",
            "[[189372   1473      0      0      0]\n",
            " [ 25532  20278   1445      0      0]\n",
            " [  1838   6970   9448    258    115]\n",
            " [    64    101   3051    710    790]\n",
            " [     7      0    374    220    922]]\n",
            "\n",
            "VAL\n",
            "F1-macro: 0.5722964331274898\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.99      0.93     31973\n",
            "           1       0.70      0.43      0.53      7809\n",
            "           2       0.64      0.51      0.57      3008\n",
            "           3       0.59      0.10      0.17       699\n",
            "           4       0.62      0.71      0.66       335\n",
            "\n",
            "    accuracy                           0.84     43824\n",
            "   macro avg       0.68      0.55      0.57     43824\n",
            "weighted avg       0.82      0.84      0.82     43824\n",
            "\n",
            "Confusion matrix:\n",
            "[[31736   237     0     0     0]\n",
            " [ 4125  3355   329     0     0]\n",
            " [  229  1200  1538    10    31]\n",
            " [   21    12   486    68   112]\n",
            " [    0     0    60    38   237]]\n",
            "\n",
            "TEST\n",
            "F1-macro: 0.5581287028354182\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.99      0.92     32219\n",
            "           1       0.70      0.37      0.48      7996\n",
            "           2       0.63      0.50      0.56      2660\n",
            "           3       0.72      0.21      0.32       740\n",
            "           4       0.40      0.69      0.51       198\n",
            "\n",
            "    accuracy                           0.83     43813\n",
            "   macro avg       0.66      0.55      0.56     43813\n",
            "weighted avg       0.82      0.83      0.81     43813\n",
            "\n",
            "Confusion matrix:\n",
            "[[31969   250     0     0     0]\n",
            " [ 4696  2953   347     0     0]\n",
            " [  293   986  1320    29    32]\n",
            " [   10     9   396   152   173]\n",
            " [    0     0    30    31   137]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import numpy.lib.stride_tricks as st\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Window length + raw feature list (no moving averages)\n",
        "# --------------------------------------------------\n",
        "W = int(W)\n",
        "COLS = [\"ws\", \"tp\", \"mslp\", \"surge\", \"twl\", \"steep\", \"temp\", \"dwd_sin\", \"dwd_cos\"]\n",
        "print(\"Using RAW features (no MAs):\", COLS)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Build sliding windows: (num_windows, W, num_features)\n",
        "# --------------------------------------------------\n",
        "arrs = [df[c].to_numpy(np.float32) for c in COLS]\n",
        "X_list = [st.sliding_window_view(a, window_shape=W) for a in arrs]\n",
        "X = np.stack(X_list, axis=-1).astype(np.float32)\n",
        "\n",
        "# Labels + split masks (assumed already prepared outside)\n",
        "y = np.asarray(y)\n",
        "train_mask = np.asarray(train_mask)\n",
        "val_mask   = np.asarray(val_mask)\n",
        "test_mask  = np.asarray(test_mask)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Align lengths (safe-guard if something is longer/shorter)\n",
        "# --------------------------------------------------\n",
        "N = min(len(X), len(y), len(train_mask), len(val_mask), len(test_mask))\n",
        "if (N != len(X)) or (N != len(y)) or (N != len(train_mask)) or (N != len(val_mask)) or (N != len(test_mask)):\n",
        "    print(f\"Aligning lengths -> X:{len(X)} y:{len(y)} \"\n",
        "          f\"train:{len(train_mask)} val:{len(val_mask)} test:{len(test_mask)} => N={N}\")\n",
        "\n",
        "X = X[:N]\n",
        "y = y[:N]\n",
        "train_mask = train_mask[:N]\n",
        "val_mask   = val_mask[:N]\n",
        "test_mask  = test_mask[:N]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Drop windows containing NaN/inf (keeps training clean)\n",
        "# --------------------------------------------------\n",
        "finite_mask = np.isfinite(X).all(axis=(1, 2))\n",
        "if not finite_mask.all():\n",
        "    print(f\"Dropping {(~finite_mask).sum()} windows due to NaN/inf.\")\n",
        "    X = X[finite_mask]\n",
        "    y = y[finite_mask]\n",
        "    train_mask = train_mask[finite_mask]\n",
        "    val_mask   = val_mask[finite_mask]\n",
        "    test_mask  = test_mask[finite_mask]\n",
        "\n",
        "# Train / val / test split\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_val,   y_val   = X[val_mask],   y[val_mask]\n",
        "X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "\n",
        "n_classes = int(np.max(y_train)) + 1\n",
        "print(\"Shapes:\",\n",
        "      \"\\n  X_train:\", X_train.shape,\n",
        "      \"\\n  X_val:  \", X_val.shape,\n",
        "      \"\\n  X_test: \", X_test.shape,\n",
        "      \"\\n  #classes:\", n_classes)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Standardization using TRAIN only (no leakage)\n",
        "# (computed over all timesteps inside train windows)\n",
        "# --------------------------------------------------\n",
        "eps = 1e-6\n",
        "flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "mu  = flat.mean(axis=0, keepdims=True)\n",
        "std = np.maximum(flat.std(axis=0, keepdims=True), eps)\n",
        "\n",
        "def standardize(Xn):\n",
        "    return (Xn - mu) / std\n",
        "\n",
        "X_train = standardize(X_train).astype(np.float32)\n",
        "X_val   = standardize(X_val).astype(np.float32)\n",
        "X_test  = standardize(X_test).astype(np.float32)\n",
        "print(\"Standardization: ON (train-only).\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# MiniROCKET expects (n_instances, n_channels, n_timepoints)\n",
        "# so we transpose: (N, W, F) -> (N, F, W)\n",
        "# --------------------------------------------------\n",
        "X_train_mr = np.transpose(X_train, (0, 2, 1)).astype(np.float32)\n",
        "X_val_mr   = np.transpose(X_val,   (0, 2, 1)).astype(np.float32)\n",
        "X_test_mr  = np.transpose(X_test,  (0, 2, 1)).astype(np.float32)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Import MiniROCKET (multivariate if available)\n",
        "# --------------------------------------------------\n",
        "try:\n",
        "    from sktime.transformations.panel.rocket import MiniRocketMultivariate as MiniRocket\n",
        "except Exception:\n",
        "    from sktime.transformations.panel.rocket import MiniRocket\n",
        "\n",
        "mr = MiniRocket(random_state=42)\n",
        "\n",
        "print(\"Fitting MiniROCKET on TRAIN ...\")\n",
        "mr.fit(X_train_mr)\n",
        "\n",
        "print(\"Transforming ...\")\n",
        "Z_train = mr.transform(X_train_mr)\n",
        "Z_val   = mr.transform(X_val_mr)\n",
        "Z_test  = mr.transform(X_test_mr)\n",
        "\n",
        "# Convert potential pandas output to numpy\n",
        "def _to_numpy(Z):\n",
        "    return Z.to_numpy() if hasattr(Z, \"to_numpy\") else np.asarray(Z)\n",
        "\n",
        "Z_train = _to_numpy(Z_train).astype(np.float32, copy=False)\n",
        "Z_val   = _to_numpy(Z_val).astype(np.float32, copy=False)\n",
        "Z_test  = _to_numpy(Z_test).astype(np.float32, copy=False)\n",
        "\n",
        "print(\"Transformed shapes:\", Z_train.shape, Z_val.shape, Z_test.shape)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Extra scaling in ROCKET feature space\n",
        "# with_mean=False is common because ROCKET features can be sparse-ish\n",
        "# --------------------------------------------------\n",
        "z_scaler = StandardScaler(with_mean=False)\n",
        "Z_train_s = z_scaler.fit_transform(Z_train)\n",
        "Z_val_s   = z_scaler.transform(Z_val)\n",
        "Z_test_s  = z_scaler.transform(Z_test)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Tune alpha (regularization strength) on validation set\n",
        "# --------------------------------------------------\n",
        "alphas = np.logspace(-6, -2, 9)\n",
        "\n",
        "best_alpha = None\n",
        "best_f1 = -1.0\n",
        "best_clf = None\n",
        "\n",
        "print(\"Selecting alpha on VAL (macro-F1) ...\")\n",
        "for a in alphas:\n",
        "    clf = SGDClassifier(\n",
        "        loss=\"log_loss\",          # logistic regression via SGD\n",
        "        alpha=float(a),           # regularization strength\n",
        "        penalty=\"l2\",\n",
        "        max_iter=2000,\n",
        "        tol=1e-3,\n",
        "        early_stopping=True,      # internal split from train for stopping\n",
        "        n_iter_no_change=5,\n",
        "        validation_fraction=0.1,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(Z_train_s, y_train.astype(int))\n",
        "    yp = clf.predict(Z_val_s)\n",
        "    f1m = f1_score(y_val.astype(int), yp, average=\"macro\")\n",
        "    print(f\"  alpha={a:.2e} | val_f1_macro={f1m:.4f}\")\n",
        "\n",
        "    # Keep the best model based on validation macro-F1\n",
        "    if f1m > best_f1 + 1e-6:\n",
        "        best_f1 = f1m\n",
        "        best_alpha = float(a)\n",
        "        best_clf = clf\n",
        "\n",
        "print(f\"Best alpha: {best_alpha:.2e} | best val_f1_macro={best_f1:.4f}\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Final evaluation on each split\n",
        "# --------------------------------------------------\n",
        "def report_split(name, Zs, y_true):\n",
        "    y_pred = best_clf.predict(Zs)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"F1-macro:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
        "    print(classification_report(y_true, y_pred, zero_division=0))\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "report_split(\"TRAIN\", Z_train_s, y_train.astype(int))\n",
        "report_split(\"VAL\",   Z_val_s,   y_val.astype(int))\n",
        "report_split(\"TEST\",  Z_test_s,  y_test.astype(int))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K20ffLusAXj1"
      },
      "source": [
        "MA'LARI EKLEYİNCE MINIROCKET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IHUHAEgAXXm",
        "outputId": "d94640d4-c749-441c-daa2-009fbe11d327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using RAW cols: ['ws', 'tp', 'mslp', 'surge', 'twl', 'steep', 'temp', 'dwd_sin', 'dwd_cos']\n",
            "Requested MA cols: ['ws_ma3', 'ws_ma6', 'ws_ma12', 'mslp_ma3', 'mslp_ma6', 'mslp_ma12', 'temp_ma3', 'temp_ma6', 'temp_ma12', 'surge_ma3', 'surge_ma6', 'surge_ma12', 'twl_ma3', 'twl_ma6', 'twl_ma12']\n",
            "Found MA cols in df: ['ws_ma3', 'ws_ma6', 'ws_ma12', 'mslp_ma3', 'mslp_ma6', 'mslp_ma12', 'temp_ma3', 'temp_ma6', 'temp_ma12', 'surge_ma3', 'surge_ma6', 'surge_ma12', 'twl_ma3', 'twl_ma6', 'twl_ma12']\n",
            "Final feature cols: ['ws', 'tp', 'mslp', 'surge', 'twl', 'steep', 'temp', 'dwd_sin', 'dwd_cos', 'ws_ma3', 'ws_ma6', 'ws_ma12', 'mslp_ma3', 'mslp_ma6', 'mslp_ma12', 'temp_ma3', 'temp_ma6', 'temp_ma12', 'surge_ma3', 'surge_ma6', 'surge_ma12', 'twl_ma3', 'twl_ma6', 'twl_ma12']\n",
            "Dropping 11 windows due to NaN/inf.\n",
            "Shapes: \n",
            "  X_train: (262957, 36, 24) \n",
            "  X_val:   (43824, 36, 24) \n",
            "  X_test:  (43813, 36, 24) \n",
            "  #classes: 5\n",
            "Standardization: ON (train-only).\n",
            "Fitting MiniROCKET on TRAIN ...\n",
            "Transforming ...\n",
            "Transformed shapes: (262957, 9996) (43824, 9996) (43813, 9996)\n",
            "Selecting alpha on VAL (macro-F1) ...\n",
            "  alpha=1.00e-06 | val_f1_macro=0.5608\n",
            "  alpha=3.16e-06 | val_f1_macro=0.5662\n",
            "  alpha=1.00e-05 | val_f1_macro=0.4786\n",
            "  alpha=3.16e-05 | val_f1_macro=0.5916\n",
            "  alpha=1.00e-04 | val_f1_macro=0.5512\n",
            "  alpha=3.16e-04 | val_f1_macro=0.5583\n",
            "  alpha=1.00e-03 | val_f1_macro=0.5459\n",
            "  alpha=3.16e-03 | val_f1_macro=0.5562\n",
            "  alpha=1.00e-02 | val_f1_macro=0.5032\n",
            "Best alpha: 3.16e-05 | best val_f1_macro=0.5916\n",
            "\n",
            "TRAIN\n",
            "F1-macro: 0.5802649683717577\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.96    190845\n",
            "           1       0.71      0.78      0.74     47244\n",
            "           2       0.67      0.57      0.62     18629\n",
            "           3       0.59      0.04      0.08      4716\n",
            "           4       0.40      0.68      0.51      1523\n",
            "\n",
            "    accuracy                           0.88    262957\n",
            "   macro avg       0.67      0.61      0.58    262957\n",
            "weighted avg       0.88      0.88      0.88    262957\n",
            "\n",
            "Confusion matrix:\n",
            "[[183316   7529      0      0      0]\n",
            " [  8942  36761   1541      0      0]\n",
            " [   136   7394  10661     46    392]\n",
            " [    10     89   3275    199   1143]\n",
            " [     0      5    394     90   1034]]\n",
            "\n",
            "VAL\n",
            "F1-macro: 0.5916486957604177\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.96     31973\n",
            "           1       0.70      0.76      0.73      7809\n",
            "           2       0.63      0.55      0.58      3008\n",
            "           3       0.57      0.02      0.05       699\n",
            "           4       0.54      0.79      0.64       335\n",
            "\n",
            "    accuracy                           0.88     43824\n",
            "   macro avg       0.68      0.62      0.59     43824\n",
            "weighted avg       0.88      0.88      0.87     43824\n",
            "\n",
            "Confusion matrix:\n",
            "[[30685  1286     2     0     0]\n",
            " [ 1467  5932   410     0     0]\n",
            " [   12  1293  1643     0    60]\n",
            " [    1    10   506    17   165]\n",
            " [    0     0    56    13   266]]\n",
            "\n",
            "TEST\n",
            "F1-macro: 0.5838408742241675\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95     32219\n",
            "           1       0.69      0.77      0.73      7996\n",
            "           2       0.63      0.52      0.57      2660\n",
            "           3       0.82      0.11      0.19       740\n",
            "           4       0.36      0.72      0.48       198\n",
            "\n",
            "    accuracy                           0.88     43813\n",
            "   macro avg       0.69      0.61      0.58     43813\n",
            "weighted avg       0.88      0.88      0.87     43813\n",
            "\n",
            "Confusion matrix:\n",
            "[[30663  1556     0     0     0]\n",
            " [ 1528  6152   316     0     0]\n",
            " [   20  1199  1386     0    55]\n",
            " [    0     6   463    78   193]\n",
            " [    0     0    39    17   142]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import numpy.lib.stride_tricks as st\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Window length\n",
        "# --------------------------------------------------\n",
        "W = int(W)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Raw (instantaneous) features\n",
        "# --------------------------------------------------\n",
        "RAW_COLS = [\"ws\", \"tp\", \"mslp\", \"surge\", \"twl\", \"steep\", \"temp\", \"dwd_sin\", \"dwd_cos\"]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Requested moving average features\n",
        "# (only a subset of variables)\n",
        "# --------------------------------------------------\n",
        "MA_BASE_COLS = [\"ws\", \"mslp\", \"temp\", \"surge\", \"twl\"]\n",
        "MA_HOURS = [3, 6, 12]\n",
        "\n",
        "MA_COLS = [f\"{c}_ma{h}\" for c in MA_BASE_COLS for h in MA_HOURS]\n",
        "\n",
        "# Keep only MA columns that actually exist in the dataframe\n",
        "existing_ma_cols = [c for c in MA_COLS if c in df.columns]\n",
        "\n",
        "# Final feature list\n",
        "COLS = RAW_COLS + existing_ma_cols\n",
        "\n",
        "print(\"Using RAW cols:\", RAW_COLS)\n",
        "print(\"Requested MA cols:\", MA_COLS)\n",
        "print(\"Found MA cols in df:\", existing_ma_cols)\n",
        "print(\"Final feature cols:\", COLS)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Build sliding windows: (num_windows, W, num_features)\n",
        "# --------------------------------------------------\n",
        "arrs = [df[c].to_numpy(np.float32) for c in COLS]\n",
        "X_list = [st.sliding_window_view(a, window_shape=W) for a in arrs]\n",
        "X = np.stack(X_list, axis=-1).astype(np.float32)\n",
        "\n",
        "# Labels and split masks (assumed prepared beforehand)\n",
        "y = np.asarray(y)\n",
        "train_mask = np.asarray(train_mask)\n",
        "val_mask   = np.asarray(val_mask)\n",
        "test_mask  = np.asarray(test_mask)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Align lengths (defensive check)\n",
        "# --------------------------------------------------\n",
        "N = min(len(X), len(y), len(train_mask), len(val_mask), len(test_mask))\n",
        "if (N != len(X)) or (N != len(y)) or (N != len(train_mask)) or (N != len(val_mask)) or (N != len(test_mask)):\n",
        "    print(f\"Aligning lengths -> X:{len(X)} y:{len(y)} \"\n",
        "          f\"train:{len(train_mask)} val:{len(val_mask)} test:{len(test_mask)} => N={N}\")\n",
        "\n",
        "X = X[:N]\n",
        "y = y[:N]\n",
        "train_mask = train_mask[:N]\n",
        "val_mask   = val_mask[:N]\n",
        "test_mask  = test_mask[:N]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Drop windows containing NaN or inf values\n",
        "# --------------------------------------------------\n",
        "finite_mask = np.isfinite(X).all(axis=(1, 2))\n",
        "if not finite_mask.all():\n",
        "    print(f\"Dropping {(~finite_mask).sum()} windows due to NaN/inf.\")\n",
        "    X = X[finite_mask]\n",
        "    y = y[finite_mask]\n",
        "    train_mask = train_mask[finite_mask]\n",
        "    val_mask   = val_mask[finite_mask]\n",
        "    test_mask  = test_mask[finite_mask]\n",
        "\n",
        "# Train / validation / test split\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_val,   y_val   = X[val_mask],   y[val_mask]\n",
        "X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "\n",
        "n_classes = int(np.max(y_train)) + 1\n",
        "print(\"Shapes:\",\n",
        "      \"\\n  X_train:\", X_train.shape,\n",
        "      \"\\n  X_val:  \", X_val.shape,\n",
        "      \"\\n  X_test: \", X_test.shape,\n",
        "      \"\\n  #classes:\", n_classes)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Standardization using TRAIN only (no leakage)\n",
        "# (statistics computed over all timesteps)\n",
        "# --------------------------------------------------\n",
        "eps = 1e-6\n",
        "flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "mu  = flat.mean(axis=0, keepdims=True)\n",
        "std = np.maximum(flat.std(axis=0, keepdims=True), eps)\n",
        "\n",
        "def standardize(Xn):\n",
        "    return (Xn - mu) / std\n",
        "\n",
        "X_train = standardize(X_train).astype(np.float32)\n",
        "X_val   = standardize(X_val).astype(np.float32)\n",
        "X_test  = standardize(X_test).astype(np.float32)\n",
        "print(\"Standardization: ON (train-only).\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# MiniROCKET expects (instances, channels, time)\n",
        "# Convert from (N, W, F) -> (N, F, W)\n",
        "# --------------------------------------------------\n",
        "X_train_mr = np.transpose(X_train, (0, 2, 1)).astype(np.float32)\n",
        "X_val_mr   = np.transpose(X_val,   (0, 2, 1)).astype(np.float32)\n",
        "X_test_mr  = np.transpose(X_test,  (0, 2, 1)).astype(np.float32)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Import MiniROCKET (multivariate if available)\n",
        "# --------------------------------------------------\n",
        "try:\n",
        "    from sktime.transformations.panel.rocket import MiniRocketMultivariate as MiniRocket\n",
        "except Exception:\n",
        "    from sktime.transformations.panel.rocket import MiniRocket\n",
        "\n",
        "mr = MiniRocket(random_state=42)\n",
        "\n",
        "print(\"Fitting MiniROCKET on TRAIN ...\")\n",
        "mr.fit(X_train_mr)\n",
        "\n",
        "print(\"Transforming ...\")\n",
        "Z_train = mr.transform(X_train_mr)\n",
        "Z_val   = mr.transform(X_val_mr)\n",
        "Z_test  = mr.transform(X_test_mr)\n",
        "\n",
        "# Convert potential pandas output to numpy\n",
        "def _to_numpy(Z):\n",
        "    return Z.to_numpy() if hasattr(Z, \"to_numpy\") else np.asarray(Z)\n",
        "\n",
        "Z_train = _to_numpy(Z_train).astype(np.float32, copy=False)\n",
        "Z_val   = _to_numpy(Z_val).astype(np.float32, copy=False)\n",
        "Z_test  = _to_numpy(Z_test).astype(np.float32, copy=False)\n",
        "\n",
        "print(\"Transformed shapes:\", Z_train.shape, Z_val.shape, Z_test.shape)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Extra scaling in ROCKET feature space\n",
        "# (with_mean=False is standard for ROCKET features)\n",
        "# --------------------------------------------------\n",
        "z_scaler = StandardScaler(with_mean=False)\n",
        "Z_train_s = z_scaler.fit_transform(Z_train)\n",
        "Z_val_s   = z_scaler.transform(Z_val)\n",
        "Z_test_s  = z_scaler.transform(Z_test)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Tune regularization strength (alpha) on validation set\n",
        "# --------------------------------------------------\n",
        "alphas = np.logspace(-6, -2, 9)\n",
        "\n",
        "best_alpha = None\n",
        "best_f1 = -1.0\n",
        "best_clf = None\n",
        "\n",
        "print(\"Selecting alpha on VAL (macro-F1) ...\")\n",
        "for a in alphas:\n",
        "    clf = SGDClassifier(\n",
        "        loss=\"log_loss\",          # logistic regression via SGD\n",
        "        alpha=float(a),           # L2 regularization strength\n",
        "        penalty=\"l2\",\n",
        "        max_iter=2000,\n",
        "        tol=1e-3,\n",
        "        early_stopping=True,\n",
        "        n_iter_no_change=5,\n",
        "        validation_fraction=0.1,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(Z_train_s, y_train.astype(int))\n",
        "    yp = clf.predict(Z_val_s)\n",
        "    f1m = f1_score(y_val.astype(int), yp, average=\"macro\")\n",
        "    print(f\"  alpha={a:.2e} | val_f1_macro={f1m:.4f}\")\n",
        "\n",
        "    # Keep the best model based on validation macro-F1\n",
        "    if f1m > best_f1 + 1e-6:\n",
        "        best_f1 = f1m\n",
        "        best_alpha = float(a)\n",
        "        best_clf = clf\n",
        "\n",
        "print(f\"Best alpha: {best_alpha:.2e} | best val_f1_macro={best_f1:.4f}\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Final evaluation on each split\n",
        "# --------------------------------------------------\n",
        "def report_split(name, Zs, y_true):\n",
        "    y_pred = best_clf.predict(Zs)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"F1-macro:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
        "    print(classification_report(y_true, y_pred, zero_division=0))\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "report_split(\"TRAIN\", Z_train_s, y_train.astype(int))\n",
        "report_split(\"VAL\",   Z_val_s,   y_val.astype(int))\n",
        "report_split(\"TEST\",  Z_test_s,  y_test.astype(int))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
