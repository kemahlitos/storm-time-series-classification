{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRfshYrNjeVw",
        "outputId": "65383383-46e3-47d1-90ee-f574288be9f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEBJS6zdjjvR",
        "outputId": "eef4299b-40c4-4688-bd01-52d3255e5c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns: ['time', 'ws', 'tp', 'hs', 'pwr', 'mslp', 'temp', 'surge', 'twl', 'steep', 'dwd_sin', 'dwd_cos', 'hs_ma3', 'hs_ma6', 'hs_ma12', 'ws_ma3', 'ws_ma6', 'ws_ma12', 'pwr_ma3', 'pwr_ma6', 'pwr_ma12', 'mslp_ma3', 'mslp_ma6', 'mslp_ma12', 'temp_ma3', 'temp_ma6', 'temp_ma12', 'surge_ma3', 'surge_ma6', 'surge_ma12', 'twl_ma3', 'twl_ma6', 'twl_ma12', 'steep_ma3', 'steep_ma6', 'steep_ma12', 'tp_ma3', 'tp_ma6', 'tp_ma12']\n",
            "Class dist:\n",
            "train {0: 0.725734690152414, 1: 0.1796986705606766, 2: 0.07084131909585957, 3: 0.017933740987496578, 4: 0.005791579203553284}\n",
            "val {0: 0.7295774005111354, 1: 0.17819003285870755, 2: 0.06863818912011684, 3: 0.015950164293537787, 4: 0.007644213216502373}\n",
            "test {0: 0.735375345217173, 1: 0.18250291009517722, 2: 0.060712573893593226, 3: 0.016889964165886836, 4: 0.0045192066281697215}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.lib.stride_tricks as st\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Paths and basic configuration\n",
        "# --------------------------------------------------\n",
        "PATH = \"/content/drive/MyDrive/Datamining-TSC-Project/new_processed_data.parquet\"\n",
        "CFG = {\n",
        "    \"time_col\": \"time\",\n",
        "    \"window\": 36,        # sliding window length (hours)\n",
        "    \"trend_h\": 12,       # recent hours for trend checks\n",
        "    \"ma_hours\": [3, 6, 12],  # moving average windows\n",
        "}\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Angle utility functions\n",
        "# --------------------------------------------------\n",
        "def wrap360(x):\n",
        "    # Wrap angles into [0, 360)\n",
        "    return (x % 360.0 + 360.0) % 360.0\n",
        "\n",
        "def angle_diff_deg(a, b):\n",
        "    # Smallest signed angle difference a - b in degrees\n",
        "    return (a - b + 180.0) % 360.0 - 180.0\n",
        "\n",
        "def wave_dir_convert(old_wave_dir):\n",
        "    # Convert wave direction to wind-direction convention\n",
        "    return wrap360(270.0 - old_wave_dir)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Moving average feature generator\n",
        "# --------------------------------------------------\n",
        "def add_moving_averages(df, cols, ma_hours):\n",
        "    # Add rolling mean features for selected columns\n",
        "    for col in cols:\n",
        "        for h in ma_hours:\n",
        "            df[f\"{col}_ma{h}\"] = df[col].rolling(window=h, min_periods=h).mean()\n",
        "    return df\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Load and clean data\n",
        "# --------------------------------------------------\n",
        "df = pd.read_parquet(PATH)\n",
        "\n",
        "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "df = df.sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "# Keep only relevant columns\n",
        "df = df[\n",
        "    [\n",
        "        \"time\",\n",
        "        \"Wind speed\",\n",
        "        \"Wind Direction\",\n",
        "        \"Wave Period\",\n",
        "        \"Wave Direction\",\n",
        "        \"Wave Height\",\n",
        "        \"Wave Power\",\n",
        "        \"Pressure\",\n",
        "        \"temperature\",\n",
        "        \"Surge Height\",\n",
        "        \"Total Water Level\",\n",
        "        \"Wave Steepness\",\n",
        "    ]\n",
        "].copy()\n",
        "\n",
        "# Rename columns to short, consistent names\n",
        "df.rename(\n",
        "    columns={\n",
        "        \"Wind speed\": \"ws\",\n",
        "        \"Wind Direction\": \"wd\",\n",
        "        \"Wave Period\": \"tp\",\n",
        "        \"Wave Direction\": \"wdir\",\n",
        "        \"Wave Height\": \"hs\",\n",
        "        \"Wave Power\": \"pwr\",\n",
        "        \"Pressure\": \"mslp\",\n",
        "        \"temperature\": \"temp\",\n",
        "        \"Surge Height\": \"surge\",\n",
        "        \"Total Water Level\": \"twl\",\n",
        "        \"Wave Steepness\": \"steep\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Windâ€“wave direction alignment features\n",
        "# --------------------------------------------------\n",
        "df[\"wdir\"] = wave_dir_convert(df[\"wdir\"].to_numpy(np.float32))\n",
        "\n",
        "wd = df[\"wd\"].to_numpy(np.float32)\n",
        "wdir = df[\"wdir\"].to_numpy(np.float32)\n",
        "\n",
        "# Angle difference between wind and wave directions\n",
        "dwd_deg = angle_diff_deg(wd, wdir).astype(np.float32)\n",
        "dwd_rad = np.deg2rad(dwd_deg).astype(np.float32)\n",
        "\n",
        "# Encode direction difference with sin/cos\n",
        "df[\"dwd_sin\"] = np.sin(dwd_rad).astype(np.float32)\n",
        "df[\"dwd_cos\"] = np.cos(dwd_rad).astype(np.float32)\n",
        "\n",
        "# Drop raw direction columns\n",
        "df.drop(columns=[\"wd\", \"wdir\"], inplace=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Add moving average features\n",
        "# --------------------------------------------------\n",
        "ma_cols = [\n",
        "    \"hs\", \"ws\", \"pwr\", \"mslp\",\n",
        "    \"temp\", \"surge\", \"twl\", \"steep\", \"tp\"\n",
        "]\n",
        "df = add_moving_averages(df, ma_cols, CFG[\"ma_hours\"])\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Sliding window statistics\n",
        "# --------------------------------------------------\n",
        "W = CFG[\"window\"]\n",
        "H = CFG[\"trend_h\"]\n",
        "\n",
        "hs   = df[\"hs\"].to_numpy(np.float32)\n",
        "pwr  = df[\"pwr\"].to_numpy(np.float32)\n",
        "mslp = df[\"mslp\"].to_numpy(np.float32)\n",
        "ws   = df[\"ws\"].to_numpy(np.float32)\n",
        "dwd_cos = df[\"dwd_cos\"].to_numpy(np.float32)\n",
        "\n",
        "# Create rolling windows\n",
        "hs_w   = st.sliding_window_view(hs,   W)\n",
        "pwr_w  = st.sliding_window_view(pwr,  W)\n",
        "mslp_w = st.sliding_window_view(mslp, W)\n",
        "ws_w   = st.sliding_window_view(ws,   W)\n",
        "dwd_cos_w = st.sliding_window_view(dwd_cos, W)\n",
        "\n",
        "# Window-based severity metrics (mean + 2*std)\n",
        "hs_metric36  = hs_w.mean(axis=1)  + 2.0 * hs_w.std(axis=1)\n",
        "pwr_metric36 = pwr_w.mean(axis=1) + 2.0 * pwr_w.std(axis=1)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Train / validation / test split by time\n",
        "# --------------------------------------------------\n",
        "start_times = df[\"time\"].iloc[:len(hs_metric36)].to_numpy()\n",
        "\n",
        "train_mask = start_times < np.datetime64(\"2015-01-01\")\n",
        "val_mask   = (start_times >= np.datetime64(\"2015-01-01\")) & (start_times < np.datetime64(\"2020-01-01\"))\n",
        "test_mask  = start_times >= np.datetime64(\"2020-01-01\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Percentile-based severity thresholds (train only)\n",
        "# --------------------------------------------------\n",
        "hs_p75, hs_p92, hs_p98, hs_p995 = np.percentile(\n",
        "    hs_metric36[train_mask], [75, 92, 98, 99.5]\n",
        ")\n",
        "pwr_p75, pwr_p92, pwr_p98, pwr_p995 = np.percentile(\n",
        "    pwr_metric36[train_mask], [75, 92, 98, 99.5]\n",
        ")\n",
        "\n",
        "# Map continuous values to severity classes\n",
        "def severity(x, p75, p92, p98, p995):\n",
        "    y = np.zeros_like(x, dtype=np.int8)\n",
        "    y[(x >= p75) & (x < p92)]  = 1\n",
        "    y[(x >= p92) & (x < p98)]  = 2\n",
        "    y[(x >= p98) & (x < p995)] = 3\n",
        "    y[(x >= p995)]             = 4\n",
        "    return y\n",
        "\n",
        "sev_hs  = severity(hs_metric36,  hs_p75,  hs_p92,  hs_p98,  hs_p995)\n",
        "sev_pwr = severity(pwr_metric36, pwr_p75, pwr_p92, pwr_p98, pwr_p995)\n",
        "\n",
        "# Base severity: worst of wave height or power\n",
        "base = np.maximum(sev_hs, sev_pwr)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Trend-based reinforcement rules (train only)\n",
        "# --------------------------------------------------\n",
        "train_hours_mask = df[\"time\"] < \"2015-01-01\"\n",
        "\n",
        "hs_th   = np.percentile(hs[train_hours_mask],  92)\n",
        "pwr_th  = np.percentile(pwr[train_hours_mask], 92)\n",
        "ws_th   = np.percentile(ws[train_hours_mask],  92)\n",
        "mslp_th = np.percentile(mslp[train_hours_mask], 20)\n",
        "align_th = np.percentile(dwd_cos[train_hours_mask], 75)\n",
        "\n",
        "# Count how many storm-like conditions persist in last H hours\n",
        "cnt = (\n",
        "    (hs_w[:, -H:]  >= hs_th).sum(axis=1)  >= 6\n",
        ").astype(int) + (\n",
        "    (pwr_w[:, -H:] >= pwr_th).sum(axis=1) >= 6\n",
        ").astype(int) + (\n",
        "    (ws_w[:, -H:]  >= ws_th).sum(axis=1)  >= 6\n",
        ").astype(int) + (\n",
        "    (mslp_w[:, -H:] <= mslp_th).sum(axis=1) >= 6\n",
        ").astype(int) + (\n",
        "    (dwd_cos_w[:, -H:] >= align_th).sum(axis=1) >= 4\n",
        ").astype(int)\n",
        "\n",
        "# Final labels (trend count can be used later if needed)\n",
        "y = base.copy()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Sanity checks\n",
        "# --------------------------------------------------\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"Class dist:\")\n",
        "for name, m in [(\"train\", train_mask), (\"val\", val_mask), (\"test\", test_mask)]:\n",
        "    print(name, pd.Series(y[m]).value_counts(normalize=True).sort_index().to_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cZL2y204mmn"
      },
      "source": [
        "CNN-WITH TEMPERATURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAPcrGp34k9K",
        "outputId": "3a4983a7-388b-47a4-e92c-19ba5eb9f45e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Shapes: \n",
            "  X_train: (262968, 36, 9) \n",
            "  X_val:   (43824, 36, 9) \n",
            "  X_test:  (43813, 36, 9)\n",
            "y_train unique: [0 1 2 3 4] dtype: int8\n",
            "Class weights (balanced): {0: 0.2755828027980822, 1: 1.1129742884350862, 2: 2.823211122443502, 3: 11.152162849872774, 4: 34.53289560078792}\n",
            "Class weights (sqrt-soft): {0: 0.5249598106503794, 1: 1.054975965809215, 2: 1.6802413881474, 3: 3.3394854169276997, 4: 5.876469654544973}\n",
            "Epoch 01 | loss=0.2644 | val_f1_macro=0.8244 | lr=5.00e-04 | pred_dist={0: 31684, 1: 8452, 2: 2891, 3: 638, 4: 159}\n",
            "Epoch 02 | loss=0.1730 | val_f1_macro=0.8781 | lr=5.00e-04 | pred_dist={0: 31621, 1: 7546, 2: 3694, 3: 699, 4: 264}\n",
            "Epoch 03 | loss=0.1478 | val_f1_macro=0.8797 | lr=5.00e-04 | pred_dist={0: 31325, 1: 8389, 2: 2807, 3: 994, 4: 309}\n",
            "Epoch 04 | loss=0.1328 | val_f1_macro=0.9054 | lr=5.00e-04 | pred_dist={0: 32227, 1: 7476, 2: 2925, 3: 919, 4: 277}\n",
            "Epoch 05 | loss=0.1220 | val_f1_macro=0.8903 | lr=5.00e-04 | pred_dist={0: 31634, 1: 7939, 2: 3296, 3: 642, 4: 313}\n",
            "Epoch 06 | loss=0.1114 | val_f1_macro=0.8492 | lr=5.00e-04 | pred_dist={0: 31739, 1: 7585, 2: 3173, 3: 818, 4: 509}\n",
            "Epoch 07 | loss=0.1089 | val_f1_macro=0.8765 | lr=5.00e-04 | pred_dist={0: 31975, 1: 8208, 2: 2381, 3: 939, 4: 321}\n",
            "Epoch 08 | loss=0.1063 | val_f1_macro=0.8921 | lr=2.50e-04 | pred_dist={0: 32122, 1: 7759, 2: 3079, 3: 608, 4: 256}\n",
            "Epoch 09 | loss=0.0825 | val_f1_macro=0.9045 | lr=2.50e-04 | pred_dist={0: 31804, 1: 8064, 2: 3016, 3: 590, 4: 350}\n",
            "Epoch 10 | loss=0.0797 | val_f1_macro=0.9051 | lr=2.50e-04 | pred_dist={0: 31764, 1: 7906, 2: 2926, 3: 827, 4: 401}\n",
            "Epoch 11 | loss=0.0769 | val_f1_macro=0.9151 | lr=2.50e-04 | pred_dist={0: 31950, 1: 7582, 2: 3265, 3: 688, 4: 339}\n",
            "Epoch 12 | loss=0.0747 | val_f1_macro=0.9088 | lr=2.50e-04 | pred_dist={0: 32136, 1: 7710, 2: 2913, 3: 717, 4: 348}\n",
            "Epoch 13 | loss=0.0711 | val_f1_macro=0.9102 | lr=2.50e-04 | pred_dist={0: 31943, 1: 7671, 2: 3035, 3: 790, 4: 385}\n",
            "Epoch 14 | loss=0.0681 | val_f1_macro=0.8901 | lr=2.50e-04 | pred_dist={0: 32237, 1: 7959, 2: 2724, 3: 671, 4: 233}\n",
            "Epoch 15 | loss=0.0690 | val_f1_macro=0.9010 | lr=1.25e-04 | pred_dist={0: 32213, 1: 7589, 2: 3005, 3: 707, 4: 310}\n",
            "Epoch 16 | loss=0.0580 | val_f1_macro=0.9101 | lr=1.25e-04 | pred_dist={0: 31885, 1: 7826, 2: 2991, 3: 728, 4: 394}\n",
            "Epoch 17 | loss=0.0558 | val_f1_macro=0.9138 | lr=1.25e-04 | pred_dist={0: 31814, 1: 8062, 2: 3011, 3: 688, 4: 249}\n",
            "Epoch 18 | loss=0.0561 | val_f1_macro=0.9139 | lr=1.25e-04 | pred_dist={0: 31991, 1: 8005, 2: 2767, 3: 752, 4: 309}\n",
            "Epoch 19 | loss=0.0537 | val_f1_macro=0.9143 | lr=6.25e-05 | pred_dist={0: 31884, 1: 8012, 2: 2951, 3: 681, 4: 296}\n",
            "Early stopping.\n",
            "\n",
            "TRAIN\n",
            "F1-macro: 0.97368815546892\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    190845\n",
            "           1       0.98      0.97      0.97     47255\n",
            "           2       0.95      0.99      0.97     18629\n",
            "           3       0.97      0.95      0.96      4716\n",
            "           4       0.95      0.98      0.97      1523\n",
            "\n",
            "    accuracy                           0.99    262968\n",
            "   macro avg       0.97      0.98      0.97    262968\n",
            "weighted avg       0.99      0.99      0.99    262968\n",
            "\n",
            "Confusion matrix:\n",
            "[[189980    865      0      0      0]\n",
            " [   583  45816    856      0      0]\n",
            " [     0     94  18436     99      0]\n",
            " [     0      0    148   4495     73]\n",
            " [     0      0      0     30   1493]]\n",
            "\n",
            "VAL\n",
            "F1-macro: 0.9150947654642557\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     31973\n",
            "           1       0.96      0.93      0.95      7809\n",
            "           2       0.88      0.96      0.92      3008\n",
            "           3       0.87      0.85      0.86       699\n",
            "           4       0.85      0.86      0.86       335\n",
            "\n",
            "    accuracy                           0.98     43824\n",
            "   macro avg       0.91      0.92      0.92     43824\n",
            "weighted avg       0.98      0.98      0.98     43824\n",
            "\n",
            "Confusion matrix:\n",
            "[[31747   226     0     0     0]\n",
            " [  203  7274   332     0     0]\n",
            " [    0    82  2881    45     0]\n",
            " [    0     0    52   597    50]\n",
            " [    0     0     0    46   289]]\n",
            "\n",
            "TEST\n",
            "F1-macro: 0.9120378829490731\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     32219\n",
            "           1       0.96      0.94      0.95      7996\n",
            "           2       0.91      0.97      0.94      2660\n",
            "           3       0.96      0.83      0.89       740\n",
            "           4       0.66      0.98      0.79       198\n",
            "\n",
            "    accuracy                           0.98     43813\n",
            "   macro avg       0.90      0.94      0.91     43813\n",
            "weighted avg       0.98      0.98      0.98     43813\n",
            "\n",
            "Confusion matrix:\n",
            "[[31928   291     0     0     0]\n",
            " [  286  7492   218     0     0]\n",
            " [    0    48  2591    21     0]\n",
            " [    0     0    24   614   102]\n",
            " [    0     0     0     3   195]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.lib.stride_tricks as st\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Device setup\n",
        "# --------------------------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Feature configuration\n",
        "# --------------------------------------------------\n",
        "COLS = [\"ws\", \"tp\", \"mslp\", \"surge\", \"twl\", \"steep\", \"temp\", \"dwd_sin\", \"dwd_cos\"]\n",
        "W = int(W)  # window length\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Training hyperparameters\n",
        "# --------------------------------------------------\n",
        "BATCH_TRAIN = 256\n",
        "BATCH_EVAL  = 1024\n",
        "EPOCHS      = 50\n",
        "PATIENCE    = 8\n",
        "\n",
        "LR           = 5e-4\n",
        "WEIGHT_DECAY = 1e-3\n",
        "DROPOUT      = 0.25\n",
        "\n",
        "PIN_MEMORY  = (device == \"cuda\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Build sliding window input tensor\n",
        "# --------------------------------------------------\n",
        "arrs = [df[c].to_numpy(np.float32) for c in COLS]\n",
        "X_list = [st.sliding_window_view(a, window_shape=W) for a in arrs]\n",
        "\n",
        "# Shape: (samples, window, features)\n",
        "X = np.stack(X_list, axis=-1).astype(np.float32)\n",
        "\n",
        "# Align labels with windows\n",
        "y = np.asarray(y)\n",
        "y = y[:len(X)]\n",
        "\n",
        "train_mask = np.asarray(train_mask)[:len(X)]\n",
        "val_mask   = np.asarray(val_mask)[:len(X)]\n",
        "test_mask  = np.asarray(test_mask)[:len(X)]\n",
        "\n",
        "# Split data\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_val,   y_val   = X[val_mask],   y[val_mask]\n",
        "X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "\n",
        "print(\"Shapes:\",\n",
        "      \"\\n  X_train:\", X_train.shape,\n",
        "      \"\\n  X_val:  \", X_val.shape,\n",
        "      \"\\n  X_test: \", X_test.shape)\n",
        "print(\"y_train unique:\", np.unique(y_train), \"dtype:\", y_train.dtype)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Normalize using TRAIN statistics only (leakage-free)\n",
        "# --------------------------------------------------\n",
        "mu  = X_train.mean(axis=(0,1), keepdims=True)\n",
        "std = X_train.std(axis=(0,1), keepdims=True) + 1e-6\n",
        "\n",
        "X_train = (X_train - mu) / std\n",
        "X_val   = (X_val   - mu) / std\n",
        "X_test  = (X_test  - mu) / std\n",
        "\n",
        "# --------------------------------------------------\n",
        "# PyTorch dataset (channels-first for Conv1D)\n",
        "# --------------------------------------------------\n",
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, X_np, y_np):\n",
        "        self.X = torch.from_numpy(X_np).permute(0, 2, 1).contiguous()\n",
        "        self.y = torch.from_numpy(y_np.astype(np.int64))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_ds = WindowDataset(X_train, y_train)\n",
        "val_ds   = WindowDataset(X_val,   y_val)\n",
        "test_ds  = WindowDataset(X_test,  y_test)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True,  num_workers=2, pin_memory=PIN_MEMORY)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_EVAL,  shuffle=False, num_workers=2, pin_memory=PIN_MEMORY)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_EVAL,  shuffle=False, num_workers=2, pin_memory=PIN_MEMORY)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Class weights (balanced + softened)\n",
        "# --------------------------------------------------\n",
        "classes = np.unique(y_train)\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
        "\n",
        "# Slightly soften class weights for stability\n",
        "cw_soft = np.sqrt(cw)\n",
        "\n",
        "n_classes = int(classes.max()) + 1\n",
        "\n",
        "class_weight = torch.ones(n_classes, dtype=torch.float32)\n",
        "for c, w in zip(classes, cw_soft):\n",
        "    class_weight[int(c)] = float(w)\n",
        "\n",
        "class_weight = class_weight.to(device)\n",
        "\n",
        "print(\"Class weights (balanced):\", {int(c): float(w) for c, w in zip(classes, cw)})\n",
        "print(\"Class weights (sqrt-soft):\", {int(c): float(w) for c, w in zip(classes, cw_soft)})\n",
        "\n",
        "# --------------------------------------------------\n",
        "# CNN building blocks\n",
        "# --------------------------------------------------\n",
        "class ConvBlock(nn.Module):\n",
        "    # Two Conv1D layers + residual skip connection\n",
        "    def __init__(self, in_ch, out_ch, k=5, p=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=k//2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size=k, padding=k//2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.skip = nn.Conv1d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + self.skip(x)\n",
        "\n",
        "class StormCNN(nn.Module):\n",
        "    # Simple 1D CNN for multivariate time series\n",
        "    def __init__(self, in_ch, n_classes, base=64, dropout=0.25):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, base, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(base),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.b1 = ConvBlock(base,    base,   k=5, p=dropout)\n",
        "        self.b2 = ConvBlock(base,    base*2, k=5, p=dropout)\n",
        "        self.b3 = ConvBlock(base*2,  base*2, k=3, p=dropout)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(base*2, base),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(base, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.b1(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.b3(x)\n",
        "        x = self.pool(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Model, loss, optimizer\n",
        "# --------------------------------------------------\n",
        "in_ch = len(COLS)\n",
        "model = StormCNN(in_ch=in_ch, n_classes=n_classes, base=64, dropout=DROPOUT).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"max\", factor=0.5, patience=3\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Evaluation helpers\n",
        "# --------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def _predict(model, loader):\n",
        "    model.eval()\n",
        "    ys, yh = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        ys.append(yb.numpy())\n",
        "        yh.append(pred)\n",
        "    return np.concatenate(ys), np.concatenate(yh)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_dist(model, loader):\n",
        "    y_true, y_pred = _predict(model, loader)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    uniq, cnt = np.unique(y_pred, return_counts=True)\n",
        "    return f1m, dict(zip(uniq.tolist(), cnt.tolist()))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# One training epoch\n",
        "# --------------------------------------------------\n",
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total += loss.item() * xb.size(0)\n",
        "    return total / len(loader.dataset)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Training loop with early stopping\n",
        "# --------------------------------------------------\n",
        "best_f1 = -1.0\n",
        "best_state = None\n",
        "bad = 0\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    loss = train_one_epoch(model, train_loader)\n",
        "    val_f1, pred_dist = eval_f1_and_dist(model, val_loader)\n",
        "    scheduler.step(val_f1)\n",
        "\n",
        "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
        "    print(f\"Epoch {ep:02d} | loss={loss:.4f} | val_f1_macro={val_f1:.4f} | lr={lr_now:.2e} | pred_dist={pred_dist}\")\n",
        "\n",
        "    if val_f1 > best_f1 + 1e-4:\n",
        "        best_f1 = val_f1\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        bad = 0\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= PATIENCE:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# Restore best model\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Final reports\n",
        "# --------------------------------------------------\n",
        "def report_split(name, loader):\n",
        "    y_true, y_pred = _predict(model, loader)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"F1-macro:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "report_split(\"TRAIN\", train_loader)\n",
        "report_split(\"VAL\",   val_loader)\n",
        "report_split(\"TEST\",  test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9735IaK4qu6"
      },
      "source": [
        "CNN WITH MA'S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHkOzjwZ6Lan",
        "outputId": "e8b37da6-5209-48ff-993e-9ea311787078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Using cols: ['ws', 'tp', 'mslp', 'surge', 'twl', 'steep', 'temp', 'dwd_sin', 'dwd_cos', 'ws_ma3', 'ws_ma6', 'ws_ma12', 'tp_ma3', 'tp_ma6', 'tp_ma12', 'mslp_ma3', 'mslp_ma6', 'mslp_ma12', 'surge_ma3', 'surge_ma6', 'surge_ma12', 'twl_ma3', 'twl_ma6', 'twl_ma12', 'steep_ma3', 'steep_ma6', 'steep_ma12', 'temp_ma3', 'temp_ma6', 'temp_ma12']\n",
            "Shapes: \n",
            "  X_train: (262968, 36, 30) \n",
            "  X_val:   (43824, 36, 30) \n",
            "  X_test:  (43813, 36, 30)\n",
            "y_train unique: [0 1 2 3 4] dtype: int8\n",
            "Dropping windows due to NaN/inf: train=11, val=0, test=0\n",
            "Class weights (balanced): {0: 0.27557127511855173, 1: 1.1131868597070527, 2: 2.8230930270009127, 3: 11.151696352841391, 4: 34.53145108338805}\n",
            "Class weights (sqrt-soft): {0: 0.5249488309526479, 1: 1.055076707972957, 2: 1.6802062453761184, 3: 3.339415570551439, 4: 5.876346746354239}\n",
            "Epoch 01 | loss=0.2770 | val_f1_macro=0.8900 | lr=5.00e-04 | pred_dist={0: 31726, 1: 7106, 2: 3979, 3: 657, 4: 356}\n",
            "Epoch 02 | loss=0.1839 | val_f1_macro=0.8234 | lr=5.00e-04 | pred_dist={0: 31478, 1: 7978, 2: 3532, 3: 684, 4: 152}\n",
            "Epoch 03 | loss=0.1582 | val_f1_macro=0.8857 | lr=5.00e-04 | pred_dist={0: 31258, 1: 8640, 2: 2846, 3: 804, 4: 276}\n",
            "Epoch 04 | loss=0.1451 | val_f1_macro=0.8898 | lr=5.00e-04 | pred_dist={0: 31726, 1: 8060, 2: 2862, 3: 878, 4: 298}\n",
            "Epoch 05 | loss=0.1361 | val_f1_macro=0.8942 | lr=5.00e-04 | pred_dist={0: 32082, 1: 7699, 2: 3067, 3: 704, 4: 272}\n",
            "Epoch 06 | loss=0.1271 | val_f1_macro=0.8952 | lr=5.00e-04 | pred_dist={0: 31617, 1: 8509, 2: 2598, 3: 809, 4: 291}\n",
            "Epoch 07 | loss=0.1176 | val_f1_macro=0.9023 | lr=5.00e-04 | pred_dist={0: 31415, 1: 8269, 2: 3078, 3: 706, 4: 356}\n",
            "Epoch 08 | loss=0.1061 | val_f1_macro=0.9055 | lr=5.00e-04 | pred_dist={0: 31768, 1: 8205, 2: 2913, 3: 692, 4: 246}\n",
            "Epoch 09 | loss=0.1083 | val_f1_macro=0.8915 | lr=5.00e-04 | pred_dist={0: 31840, 1: 8253, 2: 2853, 3: 639, 4: 239}\n",
            "Epoch 10 | loss=0.0981 | val_f1_macro=0.8863 | lr=5.00e-04 | pred_dist={0: 31301, 1: 8237, 2: 3102, 3: 761, 4: 423}\n",
            "Epoch 11 | loss=0.0964 | val_f1_macro=0.8929 | lr=5.00e-04 | pred_dist={0: 31750, 1: 7809, 2: 3279, 3: 712, 4: 274}\n",
            "Epoch 12 | loss=0.0939 | val_f1_macro=0.9049 | lr=2.50e-04 | pred_dist={0: 32520, 1: 7579, 2: 2785, 3: 641, 4: 299}\n",
            "Epoch 13 | loss=0.0748 | val_f1_macro=0.8805 | lr=2.50e-04 | pred_dist={0: 31814, 1: 7680, 2: 3117, 3: 851, 4: 362}\n",
            "Epoch 14 | loss=0.0697 | val_f1_macro=0.9059 | lr=2.50e-04 | pred_dist={0: 32175, 1: 7550, 2: 2944, 3: 856, 4: 299}\n",
            "Epoch 15 | loss=0.0687 | val_f1_macro=0.8950 | lr=2.50e-04 | pred_dist={0: 32014, 1: 7746, 2: 2987, 3: 738, 4: 339}\n",
            "Epoch 16 | loss=0.0691 | val_f1_macro=0.8968 | lr=2.50e-04 | pred_dist={0: 32029, 1: 7798, 2: 2955, 3: 773, 4: 269}\n",
            "Epoch 17 | loss=0.0653 | val_f1_macro=0.9042 | lr=2.50e-04 | pred_dist={0: 31655, 1: 8186, 2: 2919, 3: 742, 4: 322}\n",
            "Epoch 18 | loss=0.0650 | val_f1_macro=0.8872 | lr=1.25e-04 | pred_dist={0: 31611, 1: 7868, 2: 3169, 3: 870, 4: 306}\n",
            "Epoch 19 | loss=0.0537 | val_f1_macro=0.8986 | lr=1.25e-04 | pred_dist={0: 32001, 1: 7772, 2: 2913, 3: 858, 4: 280}\n",
            "Epoch 20 | loss=0.0538 | val_f1_macro=0.9056 | lr=1.25e-04 | pred_dist={0: 31863, 1: 8039, 2: 2919, 3: 730, 4: 273}\n",
            "Epoch 21 | loss=0.0529 | val_f1_macro=0.9060 | lr=1.25e-04 | pred_dist={0: 31910, 1: 7982, 2: 2894, 3: 765, 4: 273}\n",
            "Epoch 22 | loss=0.0504 | val_f1_macro=0.9009 | lr=6.25e-05 | pred_dist={0: 31849, 1: 7941, 2: 2858, 3: 853, 4: 323}\n",
            "Early stopping.\n",
            "\n",
            "TRAIN\n",
            "F1-macro: 0.9748505060714278\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00    190845\n",
            "           1       0.98      0.97      0.97     47244\n",
            "           2       0.97      0.97      0.97     18629\n",
            "           3       0.95      0.98      0.96      4716\n",
            "           4       0.96      0.98      0.97      1523\n",
            "\n",
            "    accuracy                           0.99    262957\n",
            "   macro avg       0.97      0.98      0.97    262957\n",
            "weighted avg       0.99      0.99      0.99    262957\n",
            "\n",
            "Confusion matrix:\n",
            "[[190209    636      0      0      0]\n",
            " [  1038  45665    541      0      0]\n",
            " [     0    277  18119    233      0]\n",
            " [     0      0     48   4610     58]\n",
            " [     0      0      0     26   1497]]\n",
            "\n",
            "VAL\n",
            "F1-macro: 0.9058699622647663\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     31973\n",
            "           1       0.95      0.92      0.93      7809\n",
            "           2       0.91      0.89      0.90      3008\n",
            "           3       0.74      0.91      0.82       699\n",
            "           4       0.95      0.84      0.89       335\n",
            "\n",
            "    accuracy                           0.97     43824\n",
            "   macro avg       0.91      0.91      0.91     43824\n",
            "weighted avg       0.97      0.97      0.97     43824\n",
            "\n",
            "Confusion matrix:\n",
            "[[31766   207     0     0     0]\n",
            " [  409  7170   230     0     0]\n",
            " [    0   173  2666   169     0]\n",
            " [    0     0    48   635    16]\n",
            " [    0     0     0    52   283]]\n",
            "\n",
            "TEST\n",
            "F1-macro: 0.9018194875441747\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     32219\n",
            "           1       0.94      0.91      0.93      7996\n",
            "           2       0.91      0.93      0.92      2660\n",
            "           3       0.92      0.86      0.89       740\n",
            "           4       0.66      0.96      0.78       198\n",
            "\n",
            "    accuracy                           0.97     43813\n",
            "   macro avg       0.88      0.93      0.90     43813\n",
            "weighted avg       0.97      0.97      0.97     43813\n",
            "\n",
            "Confusion matrix:\n",
            "[[31895   324     0     0     0]\n",
            " [  455  7314   227     0     0]\n",
            " [    0   141  2473    46     0]\n",
            " [    0     0     6   635    99]\n",
            " [    0     0     0     7   191]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.lib.stride_tricks as st\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Device selection\n",
        "# --------------------------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Feature setup\n",
        "# --------------------------------------------------\n",
        "BASE_COLS = [\"ws\", \"tp\", \"mslp\", \"surge\", \"twl\", \"steep\", \"temp\", \"dwd_sin\", \"dwd_cos\"]\n",
        "MA_HOURS = [3, 6, 12]\n",
        "\n",
        "# Moving average features (only for selected base vars)\n",
        "MA_BASE = [\"ws\", \"tp\", \"mslp\", \"surge\", \"twl\", \"steep\", \"temp\"]\n",
        "MA_COLS = [f\"{c}_ma{h}\" for c in MA_BASE for h in MA_HOURS]\n",
        "\n",
        "# Final feature list\n",
        "COLS = BASE_COLS + MA_COLS\n",
        "print(\"Using cols:\", COLS)\n",
        "\n",
        "# Sliding window length\n",
        "W = int(W)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Training hyperparameters\n",
        "# --------------------------------------------------\n",
        "BATCH_TRAIN = 256\n",
        "BATCH_EVAL  = 1024\n",
        "EPOCHS      = 50\n",
        "PATIENCE    = 8\n",
        "\n",
        "LR           = 5e-4\n",
        "WEIGHT_DECAY = 1e-3\n",
        "DROPOUT      = 0.25\n",
        "\n",
        "PIN_MEMORY  = (device == \"cuda\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Build sliding-window input tensor\n",
        "# --------------------------------------------------\n",
        "arrs = [df[c].to_numpy(np.float32) for c in COLS]\n",
        "X_list = [st.sliding_window_view(a, window_shape=W) for a in arrs]\n",
        "\n",
        "# Shape: (num_windows, window_length, num_features)\n",
        "X = np.stack(X_list, axis=-1).astype(np.float32)\n",
        "\n",
        "# Align labels with windows\n",
        "y = np.asarray(y)\n",
        "y = y[:len(X)]\n",
        "\n",
        "train_mask = np.asarray(train_mask)[:len(X)]\n",
        "val_mask   = np.asarray(val_mask)[:len(X)]\n",
        "test_mask  = np.asarray(test_mask)[:len(X)]\n",
        "\n",
        "# Split data\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_val,   y_val   = X[val_mask],   y[val_mask]\n",
        "X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "\n",
        "print(\"Shapes:\",\n",
        "      \"\\n  X_train:\", X_train.shape,\n",
        "      \"\\n  X_val:  \", X_val.shape,\n",
        "      \"\\n  X_test: \", X_test.shape)\n",
        "print(\"y_train unique:\", np.unique(y_train), \"dtype:\", y_train.dtype)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Drop windows containing NaN or inf\n",
        "# --------------------------------------------------\n",
        "def _finite_mask(Xnp):\n",
        "    return np.isfinite(Xnp).all(axis=(1,2))\n",
        "\n",
        "tr_ok = _finite_mask(X_train)\n",
        "va_ok = _finite_mask(X_val)\n",
        "te_ok = _finite_mask(X_test)\n",
        "\n",
        "d_tr = int((~tr_ok).sum())\n",
        "d_va = int((~va_ok).sum())\n",
        "d_te = int((~te_ok).sum())\n",
        "if (d_tr + d_va + d_te) > 0:\n",
        "    print(f\"Dropping windows due to NaN/inf: train={d_tr}, val={d_va}, test={d_te}\")\n",
        "\n",
        "X_train, y_train = X_train[tr_ok], y_train[tr_ok]\n",
        "X_val,   y_val   = X_val[va_ok],   y_val[va_ok]\n",
        "X_test,  y_test  = X_test[te_ok],  y_test[te_ok]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Normalize using TRAIN statistics only\n",
        "# --------------------------------------------------\n",
        "mu  = X_train.mean(axis=(0,1), keepdims=True)\n",
        "std = X_train.std(axis=(0,1), keepdims=True) + 1e-6\n",
        "\n",
        "X_train = (X_train - mu) / std\n",
        "X_val   = (X_val   - mu) / std\n",
        "X_test  = (X_test  - mu) / std\n",
        "\n",
        "# --------------------------------------------------\n",
        "# PyTorch Dataset (Conv1D expects channels-first)\n",
        "# --------------------------------------------------\n",
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, X_np, y_np):\n",
        "        self.X = torch.from_numpy(X_np).permute(0, 2, 1).contiguous()\n",
        "        self.y = torch.from_numpy(y_np.astype(np.int64))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_ds = WindowDataset(X_train, y_train)\n",
        "val_ds   = WindowDataset(X_val,   y_val)\n",
        "test_ds  = WindowDataset(X_test,  y_test)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True,  num_workers=2, pin_memory=PIN_MEMORY)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_EVAL,  shuffle=False, num_workers=2, pin_memory=PIN_MEMORY)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_EVAL,  shuffle=False, num_workers=2, pin_memory=PIN_MEMORY)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Class weights (balanced + softened)\n",
        "# --------------------------------------------------\n",
        "classes = np.unique(y_train)\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
        "cw_soft = np.sqrt(cw)  # softer version for stability\n",
        "\n",
        "n_classes = int(classes.max()) + 1\n",
        "class_weight = torch.ones(n_classes, dtype=torch.float32)\n",
        "for c, w in zip(classes, cw_soft):\n",
        "    class_weight[int(c)] = float(w)\n",
        "\n",
        "class_weight = class_weight.to(device)\n",
        "\n",
        "print(\"Class weights (balanced):\", {int(c): float(w) for c, w in zip(classes, cw)})\n",
        "print(\"Class weights (sqrt-soft):\", {int(c): float(w) for c, w in zip(classes, cw_soft)})\n",
        "\n",
        "# --------------------------------------------------\n",
        "# CNN building blocks\n",
        "# --------------------------------------------------\n",
        "class ConvBlock(nn.Module):\n",
        "    # Two Conv1D layers with a residual skip connection\n",
        "    def __init__(self, in_ch, out_ch, k=5, p=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=k//2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size=k, padding=k//2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.skip = nn.Conv1d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + self.skip(x)\n",
        "\n",
        "class StormCNN(nn.Module):\n",
        "    # Simple 1D CNN for multivariate time-series classification\n",
        "    def __init__(self, in_ch, n_classes, base=64, dropout=0.25):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, base, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(base),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.b1 = ConvBlock(base,    base,   k=5, p=dropout)\n",
        "        self.b2 = ConvBlock(base,    base*2, k=5, p=dropout)\n",
        "        self.b3 = ConvBlock(base*2,  base*2, k=3, p=dropout)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(base*2, base),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(base, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.b1(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.b3(x)\n",
        "        x = self.pool(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Model, loss, optimizer\n",
        "# --------------------------------------------------\n",
        "in_ch = len(COLS)\n",
        "model = StormCNN(in_ch=in_ch, n_classes=n_classes, base=64, dropout=DROPOUT).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"max\", factor=0.5, patience=3\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Evaluation helpers\n",
        "# --------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def _predict(model, loader):\n",
        "    model.eval()\n",
        "    ys, yh = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        ys.append(yb.numpy())\n",
        "        yh.append(pred)\n",
        "    return np.concatenate(ys), np.concatenate(yh)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_dist(model, loader):\n",
        "    y_true, y_pred = _predict(model, loader)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    uniq, cnt = np.unique(y_pred, return_counts=True)\n",
        "    return f1m, dict(zip(uniq.tolist(), cnt.tolist()))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Single training epoch\n",
        "# --------------------------------------------------\n",
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total += loss.item() * xb.size(0)\n",
        "    return total / len(loader.dataset)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Training loop with early stopping\n",
        "# --------------------------------------------------\n",
        "best_f1 = -1.0\n",
        "best_state = None\n",
        "bad = 0\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    loss = train_one_epoch(model, train_loader)\n",
        "    val_f1, pred_dist = eval_f1_and_dist(model, val_loader)\n",
        "    scheduler.step(val_f1)\n",
        "\n",
        "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
        "    print(f\"Epoch {ep:02d} | loss={loss:.4f} | val_f1_macro={val_f1:.4f} | lr={lr_now:.2e} | pred_dist={pred_dist}\")\n",
        "\n",
        "    if val_f1 > best_f1 + 1e-4:\n",
        "        best_f1 = val_f1\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        bad = 0\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= PATIENCE:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# Restore best model\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Final evaluation\n",
        "# --------------------------------------------------\n",
        "def report_split(name, loader):\n",
        "    y_true, y_pred = _predict(model, loader)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"F1-macro:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "report_split(\"TRAIN\", train_loader)\n",
        "report_split(\"VAL\",   val_loader)\n",
        "report_split(\"TEST\",  test_loader)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
