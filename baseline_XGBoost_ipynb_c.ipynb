{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxN0wux0l4VA",
        "outputId": "68f15b3d-ce31-4039-86dd-3c122430abcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocgdNpy4l98P",
        "outputId": "e79ed703-06dc-43c0-ca54-4fbca07e34e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns: ['time', 'ws', 'tp', 'hs', 'pwr', 'mslp', 'temp', 'surge', 'twl', 'steep', 'dwd_sin', 'dwd_cos', 'hs_ma3', 'hs_ma6', 'hs_ma12', 'ws_ma3', 'ws_ma6', 'ws_ma12', 'pwr_ma3', 'pwr_ma6', 'pwr_ma12', 'mslp_ma3', 'mslp_ma6', 'mslp_ma12', 'temp_ma3', 'temp_ma6', 'temp_ma12', 'surge_ma3', 'surge_ma6', 'surge_ma12', 'twl_ma3', 'twl_ma6', 'twl_ma12', 'steep_ma3', 'steep_ma6', 'steep_ma12', 'tp_ma3', 'tp_ma6', 'tp_ma12']\n",
            "Class dist:\n",
            "train {0: 0.725734690152414, 1: 0.1796986705606766, 2: 0.07084131909585957, 3: 0.017933740987496578, 4: 0.005791579203553284}\n",
            "val {0: 0.7295774005111354, 1: 0.17819003285870755, 2: 0.06863818912011684, 3: 0.015950164293537787, 4: 0.007644213216502373}\n",
            "test {0: 0.735375345217173, 1: 0.18250291009517722, 2: 0.060712573893593226, 3: 0.016889964165886836, 4: 0.0045192066281697215}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.lib.stride_tricks as st\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Paths and basic configuration\n",
        "# --------------------------------------------------\n",
        "PATH = \"/content/drive/MyDrive/Datamining-TSC-Project/new_processed_data.parquet\"\n",
        "CFG = {\n",
        "    \"time_col\": \"time\",\n",
        "    \"window\": 36,        # sliding window length (hours)\n",
        "    \"trend_h\": 12,       # recent hours for trend checks\n",
        "    \"ma_hours\": [3, 6, 12],  # moving average windows\n",
        "}\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Angle utility functions\n",
        "# --------------------------------------------------\n",
        "def wrap360(x):\n",
        "    # Wrap angles into [0, 360)\n",
        "    return (x % 360.0 + 360.0) % 360.0\n",
        "\n",
        "def angle_diff_deg(a, b):\n",
        "    # Smallest signed angle difference a - b in degrees\n",
        "    return (a - b + 180.0) % 360.0 - 180.0\n",
        "\n",
        "def wave_dir_convert(old_wave_dir):\n",
        "    # Convert wave direction to wind-direction convention\n",
        "    return wrap360(270.0 - old_wave_dir)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Moving average feature generator\n",
        "# --------------------------------------------------\n",
        "def add_moving_averages(df, cols, ma_hours):\n",
        "    # Add rolling mean features for selected columns\n",
        "    for col in cols:\n",
        "        for h in ma_hours:\n",
        "            df[f\"{col}_ma{h}\"] = df[col].rolling(window=h, min_periods=h).mean()\n",
        "    return df\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Load and clean data\n",
        "# --------------------------------------------------\n",
        "df = pd.read_parquet(PATH)\n",
        "\n",
        "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "df = df.sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "# Keep only relevant columns\n",
        "df = df[\n",
        "    [\n",
        "        \"time\",\n",
        "        \"Wind speed\",\n",
        "        \"Wind Direction\",\n",
        "        \"Wave Period\",\n",
        "        \"Wave Direction\",\n",
        "        \"Wave Height\",\n",
        "        \"Wave Power\",\n",
        "        \"Pressure\",\n",
        "        \"temperature\",\n",
        "        \"Surge Height\",\n",
        "        \"Total Water Level\",\n",
        "        \"Wave Steepness\",\n",
        "    ]\n",
        "].copy()\n",
        "\n",
        "# Rename columns to short, consistent names\n",
        "df.rename(\n",
        "    columns={\n",
        "        \"Wind speed\": \"ws\",\n",
        "        \"Wind Direction\": \"wd\",\n",
        "        \"Wave Period\": \"tp\",\n",
        "        \"Wave Direction\": \"wdir\",\n",
        "        \"Wave Height\": \"hs\",\n",
        "        \"Wave Power\": \"pwr\",\n",
        "        \"Pressure\": \"mslp\",\n",
        "        \"temperature\": \"temp\",\n",
        "        \"Surge Height\": \"surge\",\n",
        "        \"Total Water Level\": \"twl\",\n",
        "        \"Wave Steepness\": \"steep\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Windâ€“wave direction alignment features\n",
        "# --------------------------------------------------\n",
        "df[\"wdir\"] = wave_dir_convert(df[\"wdir\"].to_numpy(np.float32))\n",
        "\n",
        "wd = df[\"wd\"].to_numpy(np.float32)\n",
        "wdir = df[\"wdir\"].to_numpy(np.float32)\n",
        "\n",
        "# Angle difference between wind and wave directions\n",
        "dwd_deg = angle_diff_deg(wd, wdir).astype(np.float32)\n",
        "dwd_rad = np.deg2rad(dwd_deg).astype(np.float32)\n",
        "\n",
        "# Encode direction difference with sin/cos\n",
        "df[\"dwd_sin\"] = np.sin(dwd_rad).astype(np.float32)\n",
        "df[\"dwd_cos\"] = np.cos(dwd_rad).astype(np.float32)\n",
        "\n",
        "# Drop raw direction columns\n",
        "df.drop(columns=[\"wd\", \"wdir\"], inplace=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Add moving average features\n",
        "# --------------------------------------------------\n",
        "ma_cols = [\n",
        "    \"hs\", \"ws\", \"pwr\", \"mslp\",\n",
        "    \"temp\", \"surge\", \"twl\", \"steep\", \"tp\"\n",
        "]\n",
        "df = add_moving_averages(df, ma_cols, CFG[\"ma_hours\"])\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Sliding window statistics\n",
        "# --------------------------------------------------\n",
        "W = CFG[\"window\"]\n",
        "H = CFG[\"trend_h\"]\n",
        "\n",
        "hs   = df[\"hs\"].to_numpy(np.float32)\n",
        "pwr  = df[\"pwr\"].to_numpy(np.float32)\n",
        "mslp = df[\"mslp\"].to_numpy(np.float32)\n",
        "ws   = df[\"ws\"].to_numpy(np.float32)\n",
        "dwd_cos = df[\"dwd_cos\"].to_numpy(np.float32)\n",
        "\n",
        "# Create rolling windows\n",
        "hs_w   = st.sliding_window_view(hs,   W)\n",
        "pwr_w  = st.sliding_window_view(pwr,  W)\n",
        "mslp_w = st.sliding_window_view(mslp, W)\n",
        "ws_w   = st.sliding_window_view(ws,   W)\n",
        "dwd_cos_w = st.sliding_window_view(dwd_cos, W)\n",
        "\n",
        "# Window-based severity metrics (mean + 2*std)\n",
        "hs_metric36  = hs_w.mean(axis=1)  + 2.0 * hs_w.std(axis=1)\n",
        "pwr_metric36 = pwr_w.mean(axis=1) + 2.0 * pwr_w.std(axis=1)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Train / validation / test split by time\n",
        "# --------------------------------------------------\n",
        "start_times = df[\"time\"].iloc[:len(hs_metric36)].to_numpy()\n",
        "\n",
        "train_mask = start_times < np.datetime64(\"2015-01-01\")\n",
        "val_mask   = (start_times >= np.datetime64(\"2015-01-01\")) & (start_times < np.datetime64(\"2020-01-01\"))\n",
        "test_mask  = start_times >= np.datetime64(\"2020-01-01\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Percentile-based severity thresholds (train only)\n",
        "# --------------------------------------------------\n",
        "hs_p75, hs_p92, hs_p98, hs_p995 = np.percentile(\n",
        "    hs_metric36[train_mask], [75, 92, 98, 99.5]\n",
        ")\n",
        "pwr_p75, pwr_p92, pwr_p98, pwr_p995 = np.percentile(\n",
        "    pwr_metric36[train_mask], [75, 92, 98, 99.5]\n",
        ")\n",
        "\n",
        "# Map continuous values to severity classes\n",
        "def severity(x, p75, p92, p98, p995):\n",
        "    y = np.zeros_like(x, dtype=np.int8)\n",
        "    y[(x >= p75) & (x < p92)]  = 1\n",
        "    y[(x >= p92) & (x < p98)]  = 2\n",
        "    y[(x >= p98) & (x < p995)] = 3\n",
        "    y[(x >= p995)]             = 4\n",
        "    return y\n",
        "\n",
        "sev_hs  = severity(hs_metric36,  hs_p75,  hs_p92,  hs_p98,  hs_p995)\n",
        "sev_pwr = severity(pwr_metric36, pwr_p75, pwr_p92, pwr_p98, pwr_p995)\n",
        "\n",
        "# Base severity: worst of wave height or power\n",
        "base = np.maximum(sev_hs, sev_pwr)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Trend-based reinforcement rules (train only)\n",
        "# --------------------------------------------------\n",
        "train_hours_mask = df[\"time\"] < \"2015-01-01\"\n",
        "\n",
        "hs_th   = np.percentile(hs[train_hours_mask],  92)\n",
        "pwr_th  = np.percentile(pwr[train_hours_mask], 92)\n",
        "ws_th   = np.percentile(ws[train_hours_mask],  92)\n",
        "mslp_th = np.percentile(mslp[train_hours_mask], 20)\n",
        "align_th = np.percentile(dwd_cos[train_hours_mask], 75)\n",
        "\n",
        "# Count how many storm-like conditions persist in last H hours\n",
        "cnt = (\n",
        "    (hs_w[:, -H:]  >= hs_th).sum(axis=1)  >= 6\n",
        ").astype(int) + (\n",
        "    (pwr_w[:, -H:] >= pwr_th).sum(axis=1) >= 6\n",
        ").astype(int) + (\n",
        "    (ws_w[:, -H:]  >= ws_th).sum(axis=1)  >= 6\n",
        ").astype(int) + (\n",
        "    (mslp_w[:, -H:] <= mslp_th).sum(axis=1) >= 6\n",
        ").astype(int) + (\n",
        "    (dwd_cos_w[:, -H:] >= align_th).sum(axis=1) >= 4\n",
        ").astype(int)\n",
        "\n",
        "# Final labels (trend count can be used later if needed)\n",
        "y = base.copy()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Sanity checks\n",
        "# --------------------------------------------------\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"Class dist:\")\n",
        "for name, m in [(\"train\", train_mask), (\"val\", val_mask), (\"test\", test_mask)]:\n",
        "    print(name, pd.Series(y[m]).value_counts(normalize=True).sort_index().to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws-TtjOxl9xz",
        "outputId": "4890e86a-7eab-4a08-831f-e746a5291586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using features (hs/pwr excluded, temperature included):\n",
            "['ws', 'tp', 'mslp', 'surge', 'twl', 'steep', 'temp', 'dwd_sin', 'dwd_cos', 'ws_ma3', 'ws_ma6', 'ws_ma12', 'tp_ma3', 'tp_ma6', 'tp_ma12', 'mslp_ma3', 'mslp_ma6', 'mslp_ma12', 'surge_ma3', 'surge_ma6', 'surge_ma12', 'twl_ma3', 'twl_ma6', 'twl_ma12', 'steep_ma3', 'steep_ma6', 'steep_ma12', 'temp_ma3', 'temp_ma6', 'temp_ma12']\n",
            "Dropping 11 windows due to NaN/inf.\n",
            "Shapes: \n",
            "  X_train: (262957, 36, 30) \n",
            "  X_val:   (43824, 36, 30) \n",
            "  X_test:  (43813, 36, 30) \n",
            "  #classes: 5\n",
            "Standardization: ON (train-only).\n",
            "Flattened shapes: \n",
            "  Xtr: (262957, 1080) \n",
            "  Xva: (43824, 1080) \n",
            "  Xte: (43813, 1080)\n",
            "Class weights: {0: 0.27557127511855173, 1: 1.1131868597070527, 2: 2.8230930270009127, 3: 11.151696352841391, 4: 34.53145108338805}\n",
            "Sample weights stats: 0.2755712866783142 0.9999998807907104 34.53145217895508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [17:53:03] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"lambda_\" } are not used.\n",
            "\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:1.57355\tval-mlogloss:1.56168\n",
            "[100]\ttrain-mlogloss:0.49149\tval-mlogloss:0.38416\n",
            "[200]\ttrain-mlogloss:0.27407\tval-mlogloss:0.22602\n",
            "[300]\ttrain-mlogloss:0.18382\tval-mlogloss:0.17194\n",
            "[400]\ttrain-mlogloss:0.13694\tval-mlogloss:0.14753\n",
            "[500]\ttrain-mlogloss:0.10721\tval-mlogloss:0.13263\n",
            "[600]\ttrain-mlogloss:0.08626\tval-mlogloss:0.12240\n",
            "[700]\ttrain-mlogloss:0.07160\tval-mlogloss:0.11546\n",
            "[800]\ttrain-mlogloss:0.06046\tval-mlogloss:0.11029\n",
            "[900]\ttrain-mlogloss:0.05144\tval-mlogloss:0.10605\n",
            "[1000]\ttrain-mlogloss:0.04396\tval-mlogloss:0.10266\n",
            "[1100]\ttrain-mlogloss:0.03800\tval-mlogloss:0.10002\n",
            "[1200]\ttrain-mlogloss:0.03307\tval-mlogloss:0.09790\n",
            "[1300]\ttrain-mlogloss:0.02898\tval-mlogloss:0.09596\n",
            "[1400]\ttrain-mlogloss:0.02544\tval-mlogloss:0.09462\n",
            "[1500]\ttrain-mlogloss:0.02258\tval-mlogloss:0.09358\n",
            "[1600]\ttrain-mlogloss:0.02006\tval-mlogloss:0.09245\n",
            "[1700]\ttrain-mlogloss:0.01792\tval-mlogloss:0.09171\n",
            "[1800]\ttrain-mlogloss:0.01599\tval-mlogloss:0.09097\n",
            "[1900]\ttrain-mlogloss:0.01435\tval-mlogloss:0.09051\n",
            "[2000]\ttrain-mlogloss:0.01301\tval-mlogloss:0.09016\n",
            "[2100]\ttrain-mlogloss:0.01180\tval-mlogloss:0.09000\n",
            "[2200]\ttrain-mlogloss:0.01070\tval-mlogloss:0.08982\n",
            "[2300]\ttrain-mlogloss:0.00978\tval-mlogloss:0.08970\n",
            "[2400]\ttrain-mlogloss:0.00896\tval-mlogloss:0.08962\n",
            "[2500]\ttrain-mlogloss:0.00824\tval-mlogloss:0.08960\n",
            "[2600]\ttrain-mlogloss:0.00759\tval-mlogloss:0.08955\n",
            "[2700]\ttrain-mlogloss:0.00698\tval-mlogloss:0.08957\n",
            "[2800]\ttrain-mlogloss:0.00644\tval-mlogloss:0.08970\n",
            "[2810]\ttrain-mlogloss:0.00638\tval-mlogloss:0.08973\n",
            "Best iteration: 2610\n",
            "\n",
            "TRAIN\n",
            "F1-macro: 0.9994279199296443\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    190845\n",
            "           1       1.00      1.00      1.00     47244\n",
            "           2       1.00      1.00      1.00     18629\n",
            "           3       1.00      1.00      1.00      4716\n",
            "           4       1.00      1.00      1.00      1523\n",
            "\n",
            "    accuracy                           1.00    262957\n",
            "   macro avg       1.00      1.00      1.00    262957\n",
            "weighted avg       1.00      1.00      1.00    262957\n",
            "\n",
            "Confusion matrix:\n",
            "[[190633    212      0      0      0]\n",
            " [     5  47239      0      0      0]\n",
            " [     0      0  18629      0      0]\n",
            " [     0      0      0   4716      0]\n",
            " [     0      0      0      0   1523]]\n",
            "\n",
            "VAL\n",
            "F1-macro: 0.8795387013937663\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     31973\n",
            "           1       0.92      0.93      0.93      7809\n",
            "           2       0.84      0.91      0.87      3008\n",
            "           3       0.78      0.68      0.72       699\n",
            "           4       0.99      0.81      0.89       335\n",
            "\n",
            "    accuracy                           0.97     43824\n",
            "   macro avg       0.90      0.86      0.88     43824\n",
            "weighted avg       0.97      0.97      0.97     43824\n",
            "\n",
            "Confusion matrix:\n",
            "[[31566   407     0     0     0]\n",
            " [  243  7263   303     0     0]\n",
            " [    0   209  2727    72     0]\n",
            " [    0     0   223   473     3]\n",
            " [    0     0     0    65   270]]\n",
            "\n",
            "TEST\n",
            "F1-macro: 0.8744476749685465\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     32219\n",
            "           1       0.91      0.93      0.92      7996\n",
            "           2       0.87      0.90      0.88      2660\n",
            "           3       0.88      0.78      0.83       740\n",
            "           4       0.79      0.73      0.76       198\n",
            "\n",
            "    accuracy                           0.96     43813\n",
            "   macro avg       0.89      0.86      0.87     43813\n",
            "weighted avg       0.97      0.96      0.96     43813\n",
            "\n",
            "Confusion matrix:\n",
            "[[31699   520     0     0     0]\n",
            " [  292  7469   235     0     0]\n",
            " [    0   254  2383    23     0]\n",
            " [    0     0   125   576    39]\n",
            " [    0     0     0    54   144]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import numpy.lib.stride_tricks as st\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Reproducibility\n",
        "# --------------------------------------------------\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Sliding window length\n",
        "W = int(W)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Preprocessing options\n",
        "# --------------------------------------------------\n",
        "USE_STANDARDIZE = True\n",
        "eps = 1e-6\n",
        "\n",
        "# --------------------------------------------------\n",
        "# XGBoost training configuration\n",
        "# --------------------------------------------------\n",
        "XGB_TRAIN_PARAMS = dict(\n",
        "    objective=\"multi:softprob\",   # multi-class probabilities\n",
        "    eval_metric=\"mlogloss\",\n",
        "    num_class=None,               # set later\n",
        "    eta=0.03,                     # learning rate\n",
        "    max_depth=6,\n",
        "    min_child_weight=2,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    lambda_=1.0,\n",
        "    alpha=0.0,\n",
        "    gamma=0.0,\n",
        "    tree_method=\"hist\",           # fast CPU histogram method\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "NUM_BOOST_ROUND = 4000\n",
        "EARLY_STOPPING_ROUNDS = 200\n",
        "VERBOSE_EVAL = 100\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Base features (hs/pwr excluded, temperature included)\n",
        "# --------------------------------------------------\n",
        "BASE_COLS = [\"ws\", \"tp\", \"mslp\", \"surge\", \"twl\", \"steep\", \"temp\", \"dwd_sin\", \"dwd_cos\"]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Moving average features (only if available)\n",
        "# --------------------------------------------------\n",
        "MA_HOURS = [3, 6, 12]\n",
        "MA_COLS = []\n",
        "for c in [\"ws\", \"tp\", \"mslp\", \"surge\", \"twl\", \"steep\", \"temp\"]:\n",
        "    for h in MA_HOURS:\n",
        "        name = f\"{c}_ma{h}\"\n",
        "        if name in df.columns:\n",
        "            MA_COLS.append(name)\n",
        "\n",
        "# Final feature list\n",
        "COLS = BASE_COLS + MA_COLS\n",
        "print(\"Using features (hs/pwr excluded, temperature included):\")\n",
        "print(COLS)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Build sliding windows: (num_windows, W, num_features)\n",
        "# --------------------------------------------------\n",
        "arrs = [df[c].to_numpy(np.float32) for c in COLS]\n",
        "X_list = [st.sliding_window_view(a, window_shape=W) for a in arrs]\n",
        "X = np.stack(X_list, axis=-1).astype(np.float32)\n",
        "\n",
        "# Align labels and masks with windows\n",
        "y = np.asarray(y)[:len(X)]\n",
        "train_mask = np.asarray(train_mask)[:len(X)]\n",
        "val_mask   = np.asarray(val_mask)[:len(X)]\n",
        "test_mask  = np.asarray(test_mask)[:len(X)]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Drop windows with NaN or inf values\n",
        "# --------------------------------------------------\n",
        "finite_mask = np.isfinite(X).all(axis=(1, 2))\n",
        "if not finite_mask.all():\n",
        "    print(f\"Dropping {(~finite_mask).sum()} windows due to NaN/inf.\")\n",
        "    X = X[finite_mask]\n",
        "    y = y[finite_mask]\n",
        "    train_mask = train_mask[finite_mask]\n",
        "    val_mask   = val_mask[finite_mask]\n",
        "    test_mask  = test_mask[finite_mask]\n",
        "\n",
        "# Train / val / test split\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_val,   y_val   = X[val_mask],   y[val_mask]\n",
        "X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "\n",
        "n_classes = int(np.max(y_train)) + 1\n",
        "print(\"Shapes:\",\n",
        "      \"\\n  X_train:\", X_train.shape,\n",
        "      \"\\n  X_val:  \", X_val.shape,\n",
        "      \"\\n  X_test: \", X_test.shape,\n",
        "      \"\\n  #classes:\", n_classes)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Optional standardization (fit on TRAIN only)\n",
        "# --------------------------------------------------\n",
        "if USE_STANDARDIZE:\n",
        "    # Compute stats over all time steps in TRAIN\n",
        "    flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "    mu  = flat.mean(axis=0, keepdims=True)\n",
        "    std = np.maximum(flat.std(axis=0, keepdims=True), eps)\n",
        "\n",
        "    def standardize(Xn):\n",
        "        return (Xn - mu) / std\n",
        "\n",
        "    X_train = standardize(X_train).astype(np.float32)\n",
        "    X_val   = standardize(X_val).astype(np.float32)\n",
        "    X_test  = standardize(X_test).astype(np.float32)\n",
        "    print(\"Standardization: ON (train-only).\")\n",
        "else:\n",
        "    print(\"Standardization: OFF.\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Flatten windows for XGBoost (expects 2D input)\n",
        "# --------------------------------------------------\n",
        "Xtr = X_train.reshape(X_train.shape[0], -1)\n",
        "Xva = X_val.reshape(X_val.shape[0], -1)\n",
        "Xte = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "print(\"Flattened shapes:\",\n",
        "      \"\\n  Xtr:\", Xtr.shape,\n",
        "      \"\\n  Xva:\", Xva.shape,\n",
        "      \"\\n  Xte:\", Xte.shape)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Class weights and per-sample weights\n",
        "# --------------------------------------------------\n",
        "classes = np.unique(y_train)\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
        "cw_map = {int(c): float(w) for c, w in zip(classes, cw)}\n",
        "w_train = np.array([cw_map[int(yy)] for yy in y_train], dtype=np.float32)\n",
        "\n",
        "print(\"Class weights:\", cw_map)\n",
        "print(\"Sample weights stats:\", float(w_train.min()), float(w_train.mean()), float(w_train.max()))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Import XGBoost\n",
        "# --------------------------------------------------\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    raise ImportError(\n",
        "        \"xgboost could not be imported. It is usually available in Colab; otherwise `pip install xgboost`.\\n\"\n",
        "        f\"Original error: {e}\"\n",
        "    )\n",
        "\n",
        "# Fill num_class dynamically\n",
        "params = dict(XGB_TRAIN_PARAMS)\n",
        "params[\"num_class\"] = n_classes\n",
        "\n",
        "# Build DMatrix objects\n",
        "dtrain = xgb.DMatrix(Xtr, label=y_train.astype(np.int32), weight=w_train)\n",
        "dval   = xgb.DMatrix(Xva, label=y_val.astype(np.int32))\n",
        "dtest  = xgb.DMatrix(Xte, label=y_test.astype(np.int32))\n",
        "\n",
        "evals = [(dtrain, \"train\"), (dval, \"val\")]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Train XGBoost with early stopping\n",
        "# --------------------------------------------------\n",
        "bst = xgb.train(\n",
        "    params=params,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=NUM_BOOST_ROUND,\n",
        "    evals=evals,\n",
        "    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
        "    verbose_eval=VERBOSE_EVAL\n",
        ")\n",
        "\n",
        "best_ntree = getattr(bst, \"best_iteration\", None)\n",
        "if best_ntree is not None:\n",
        "    print(\"Best iteration:\", int(best_ntree))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Prediction helpers\n",
        "# --------------------------------------------------\n",
        "def predict_labels(dmat):\n",
        "    proba = bst.predict(dmat)\n",
        "    # Multi-class: take argmax\n",
        "    if proba.ndim == 1:\n",
        "        return (proba > 0.5).astype(int)\n",
        "    return np.argmax(proba, axis=1).astype(int)\n",
        "\n",
        "def report_split(name, dmat, ys_true):\n",
        "    yp = predict_labels(dmat)\n",
        "    ys_true = ys_true.astype(int)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"F1-macro:\", f1_score(ys_true, yp, average=\"macro\"))\n",
        "    print(classification_report(ys_true, yp, zero_division=0))\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(ys_true, yp))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Final reports\n",
        "# --------------------------------------------------\n",
        "report_split(\"TRAIN\", dtrain, y_train)\n",
        "report_split(\"VAL\",   dval,   y_val)\n",
        "report_split(\"TEST\",  dtest,  y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BDsK7cymF1d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
